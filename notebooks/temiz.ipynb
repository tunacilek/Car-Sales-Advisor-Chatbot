{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.76 in /home/train/.local/lib/python3.12/site-packages (from langchain-openai) (0.3.76)\n",
      "Collecting openai<2.0.0,>=1.104.2 (from langchain-openai)\n",
      "  Using cached openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
      "  Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /home/train/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.29)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/train/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/train/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/train/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/train/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /home/train/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /home/train/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.11.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/train/.local/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/train/.local/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.104.2->langchain-openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/train/.local/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.104.2->langchain-openai)\n",
      "  Using cached jiter-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /home/train/.local/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/train/.local/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/train/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.104.2->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/train/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/train/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/train/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/train/.local/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/train/.local/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/train/.local/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/train/.local/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/train/.local/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.5)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /home/train/.local/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/train/.local/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/train/.local/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain-openai) (0.25.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/train/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/train/.local/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
      "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
      "Using cached openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached jiter-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (347 kB)\n",
      "Downloading tiktoken-0.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jiter, distro, tiktoken, openai, langchain-openai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [langchain-openai][openai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 jiter-0.11.0 langchain-openai-0.3.33 openai-1.109.1 tiktoken-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, uuid\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client.models import Filter, FieldCondition, Range, MatchValue\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range\n",
    "from unidecode import unidecode\n",
    "\n",
    "from typing import Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97caac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "993982cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Veri boyutu: (65839, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>baslik</th>\n",
       "      <th>konum</th>\n",
       "      <th>fiyat</th>\n",
       "      <th>aciklama</th>\n",
       "      <th>marka</th>\n",
       "      <th>seri</th>\n",
       "      <th>model</th>\n",
       "      <th>yil</th>\n",
       "      <th>kilometre</th>\n",
       "      <th>yakit_tipi</th>\n",
       "      <th>vites_tipi</th>\n",
       "      <th>renk</th>\n",
       "      <th>arac_durumu</th>\n",
       "      <th>kasa_tipi</th>\n",
       "      <th>cekis</th>\n",
       "      <th>motor_hacmi</th>\n",
       "      <th>motor_gucu</th>\n",
       "      <th>url</th>\n",
       "      <th>Araç_Yası</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>YAVUZLAR'DAN 2014 ALFA ROMEO GİULİETTA 1.6 JTD...</td>\n",
       "      <td>Karşıyaka Mh. Kepez, Antalya</td>\n",
       "      <td>655000</td>\n",
       "      <td>\\n</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Giulietta</td>\n",
       "      <td>1.6 JTD Distinctive</td>\n",
       "      <td>2014</td>\n",
       "      <td>209000</td>\n",
       "      <td>Dizel</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Beyaz</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Hatchback/5</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1598</td>\n",
       "      <td>105</td>\n",
       "      <td>https://www.arabam.com/ilan/galeriden-satilik-...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2006 ALFA ROMEO 156 TS</td>\n",
       "      <td>Soğanlı Mh. Osmangazi, Bursa</td>\n",
       "      <td>414500</td>\n",
       "      <td>ES ES OTOMOTİV DEN \\n\\n\\n \\n\\n\\nSATILIK \\n\\n\\n...</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>156</td>\n",
       "      <td>1.6 TS Distinctive</td>\n",
       "      <td>2006</td>\n",
       "      <td>171000</td>\n",
       "      <td>Benzin</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Şampanya</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1600</td>\n",
       "      <td>125</td>\n",
       "      <td>https://www.arabam.com/ilan/galeriden-satilik-...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Sahibinden Alfa Romeo Giulietta 1.4 TB MultiAi...</td>\n",
       "      <td>Karşıyaka Mh. Karataş, Adana</td>\n",
       "      <td>735000</td>\n",
       "      <td>Ev almayı düşündüğüm için aracımı satışa çıkar...</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Giulietta</td>\n",
       "      <td>1.4 TB MultiAir Distinctive</td>\n",
       "      <td>2011</td>\n",
       "      <td>157100</td>\n",
       "      <td>Benzin</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Siyah</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Hatchback/5</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1368</td>\n",
       "      <td>170</td>\n",
       "      <td>https://www.arabam.com/ilan/sahibinden-satilik...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sahibinden Alfa Romeo Giulietta 1.4 TB Progres...</td>\n",
       "      <td>Esenkent Mh. Esenyurt, İstanbul</td>\n",
       "      <td>900000</td>\n",
       "      <td>-2016 NİSAN ÇIKIŞLIDIR.-ORJİNAL 90 BİN KM, SIF...</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Giulietta</td>\n",
       "      <td>1.4 TB Progression Plus</td>\n",
       "      <td>2015</td>\n",
       "      <td>90800</td>\n",
       "      <td>Benzin</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Kırmızı</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Hatchback/5</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1368</td>\n",
       "      <td>120</td>\n",
       "      <td>https://www.arabam.com/ilan/sahibinden-satilik...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A L F İ S T</td>\n",
       "      <td>Deliktaş Mh. Pamukkale, Denizli</td>\n",
       "      <td>800000</td>\n",
       "      <td></td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Giulietta</td>\n",
       "      <td>1.6 JTD Distinctive</td>\n",
       "      <td>2014</td>\n",
       "      <td>200000</td>\n",
       "      <td>Dizel</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Beyaz</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Hatchback/5</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1598</td>\n",
       "      <td>105</td>\n",
       "      <td>https://www.arabam.com/ilan/sahibinden-satilik...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             baslik  \\\n",
       "0   1  YAVUZLAR'DAN 2014 ALFA ROMEO GİULİETTA 1.6 JTD...   \n",
       "1   2                             2006 ALFA ROMEO 156 TS   \n",
       "2   3  Sahibinden Alfa Romeo Giulietta 1.4 TB MultiAi...   \n",
       "3   4  Sahibinden Alfa Romeo Giulietta 1.4 TB Progres...   \n",
       "4   5                                        A L F İ S T   \n",
       "\n",
       "                             konum   fiyat  \\\n",
       "0     Karşıyaka Mh. Kepez, Antalya  655000   \n",
       "1     Soğanlı Mh. Osmangazi, Bursa  414500   \n",
       "2     Karşıyaka Mh. Karataş, Adana  735000   \n",
       "3  Esenkent Mh. Esenyurt, İstanbul  900000   \n",
       "4  Deliktaş Mh. Pamukkale, Denizli  800000   \n",
       "\n",
       "                                            aciklama       marka       seri  \\\n",
       "0                                                 \\n  Alfa Romeo  Giulietta   \n",
       "1  ES ES OTOMOTİV DEN \\n\\n\\n \\n\\n\\nSATILIK \\n\\n\\n...  Alfa Romeo        156   \n",
       "2  Ev almayı düşündüğüm için aracımı satışa çıkar...  Alfa Romeo  Giulietta   \n",
       "3  -2016 NİSAN ÇIKIŞLIDIR.-ORJİNAL 90 BİN KM, SIF...  Alfa Romeo  Giulietta   \n",
       "4                                                     Alfa Romeo  Giulietta   \n",
       "\n",
       "                         model   yil  kilometre yakit_tipi vites_tipi  \\\n",
       "0          1.6 JTD Distinctive  2014     209000      Dizel        Düz   \n",
       "1           1.6 TS Distinctive  2006     171000     Benzin        Düz   \n",
       "2  1.4 TB MultiAir Distinctive  2011     157100     Benzin        Düz   \n",
       "3      1.4 TB Progression Plus  2015      90800     Benzin        Düz   \n",
       "4          1.6 JTD Distinctive  2014     200000      Dizel        Düz   \n",
       "\n",
       "       renk arac_durumu    kasa_tipi        cekis  motor_hacmi  motor_gucu  \\\n",
       "0     Beyaz   İkinci El  Hatchback/5  Önden Çekiş         1598         105   \n",
       "1  Şampanya   İkinci El        Sedan  Önden Çekiş         1600         125   \n",
       "2     Siyah   İkinci El  Hatchback/5  Önden Çekiş         1368         170   \n",
       "3   Kırmızı   İkinci El  Hatchback/5  Önden Çekiş         1368         120   \n",
       "4     Beyaz   İkinci El  Hatchback/5  Önden Çekiş         1598         105   \n",
       "\n",
       "                                                 url  Araç_Yası  \n",
       "0  https://www.arabam.com/ilan/galeriden-satilik-...         11  \n",
       "1  https://www.arabam.com/ilan/galeriden-satilik-...         19  \n",
       "2  https://www.arabam.com/ilan/sahibinden-satilik...         14  \n",
       "3  https://www.arabam.com/ilan/sahibinden-satilik...         10  \n",
       "4  https://www.arabam.com/ilan/sahibinden-satilik...         11  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uğur burda df'i okut\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"../data/arabam_ilanlar.parquet\")\n",
    "\n",
    "print(\"✅ Veri boyutu:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c430e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_lower(s: str) -> str:\n",
    "    return unidecode(str(s or \"\")).strip().lower()\n",
    "\n",
    "def to_num(text) -> Optional[float]:\n",
    "    if text is None: return None\n",
    "    s = str(text).lower().replace(\"tl\",\"\").replace(\"₺\",\"\").replace(\"km\",\"\").strip()\n",
    "    s = s.replace(\".\",\"\").replace(\" \",\"\").replace(\",\",\".\")\n",
    "    try: return float(s)\n",
    "    except: return None\n",
    "\n",
    "def year4(x) -> Optional[int]:\n",
    "    m = re.search(r\"\\b(19|20)\\d{2}\\b\", str(x))\n",
    "    return int(m.group(0)) if m else None\n",
    "\n",
    "def build_doc_text(row: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Embedding'e gidecek doğal dil \"ilan profili\".\n",
    "    Kısa, tek satır ya da 2-3 cümle idealdir.\n",
    "    \"\"\"\n",
    "    marka = row.get(\"marka\",\"\").strip()\n",
    "    seri = row.get(\"seri\",\"\").strip()\n",
    "    model = row.get(\"model\",\"\").strip()\n",
    "    yil = row.get(\"yil\") or row.get(\"yil_num\")\n",
    "    vites = row.get(\"vites_tipi\",\"\")\n",
    "    yakit = row.get(\"yakit_tipi\",\"\")\n",
    "    km = row.get(\"kilometre\",\"\")\n",
    "    konum = row.get(\"konum\",\"\")\n",
    "    kasa = row.get(\"kasa_tipi\",\"\")\n",
    "    fiyat = row.get(\"fiyat\",\"\")\n",
    "    aciklama = (row.get(\"aciklama\") or \"\").strip()\n",
    "\n",
    "    title = f\"{marka} {seri} {model} {yil or ''}\".strip()\n",
    "    bullet = f\"{yakit}, {vites}, {km} km, {kasa}, {konum}\".replace(\"  \",\" \").strip(\" ,\")\n",
    "    text = f\"{title} – {bullet}. Fiyat: {fiyat}. {aciklama}\"\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1166477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_point_id(raw):\n",
    "    \"\"\"\n",
    "    raw -> int (>=0) ya da UUID(str). Diğer her şey yeni UUID olur.\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "    # önce integer dene\n",
    "    try:\n",
    "        # numpy int / float gelebilir\n",
    "        if isinstance(raw, float):\n",
    "            if math.isnan(raw):\n",
    "                return str(uuid.uuid4())\n",
    "            # 1.0 gibi ise int'e çevir\n",
    "            if float(raw).is_integer() and int(raw) >= 0:\n",
    "                return int(raw)\n",
    "            else:\n",
    "                return str(uuid.uuid4())\n",
    "        iv = int(raw)\n",
    "        if iv >= 0:\n",
    "            return iv\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # sonra UUID dene\n",
    "    try:\n",
    "        return str(uuid.UUID(str(raw)))\n",
    "    except Exception:\n",
    "        # en son yeni UUID üret\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "def df_to_points(df: pd.DataFrame, embedder: OpenAIEmbeddings, collection: str, client: QdrantClient, batch_size: int = 256):\n",
    "    # Varsa önceden normalize edilmiş kolonları uygula:\n",
    "    if \"fiyat_num\" not in df.columns:\n",
    "        df[\"fiyat_num\"] = df[\"fiyat\"].map(to_num)\n",
    "    if \"km_num\" not in df.columns:\n",
    "        df[\"km_num\"] = df[\"kilometre\"].map(to_num)\n",
    "    if \"yil_num\" not in df.columns:\n",
    "        df[\"yil_num\"] = df[\"yil\"].map(year4)\n",
    "\n",
    "    # Koleksiyon oluştur\n",
    "    dim = embedder.dimension()  # encode(\"test\") yerine güvenli yol\n",
    "\n",
    "    # Koleksiyon yoksa oluştur:\n",
    "    from qdrant_client.models import Distance, VectorParams\n",
    "    if collection not in [c.name for c in client.get_collections().collections]:\n",
    "        client.create_collection(\n",
    "            collection_name=collection,\n",
    "            vectors_config=VectorParams(size=dim, distance=Distance.COSINE),\n",
    "        )\n",
    "\n",
    "    # Noktalar\n",
    "    # TODO: Burdaki değişkenleri kontrol et\n",
    "    rows = df.to_dict(orient=\"records\")\n",
    "    for i in range(0, len(rows), batch_size):\n",
    "        chunk = rows[i:i+batch_size]\n",
    "        texts = [build_doc_text(r) for r in chunk]\n",
    "        vecs = embedder.embed_documents(texts)\n",
    "\n",
    "        points = []\n",
    "        for r, v, t in zip(chunk, vecs, texts):\n",
    "            pid = make_point_id(r.get(\"id\"))\n",
    "            payload = {\n",
    "                # ham alanlar\n",
    "                \"id\": r.get(\"id\"),\n",
    "                \"baslik\": r.get(\"baslik\"),\n",
    "                \"konum\": r.get(\"konum\"),\n",
    "                \"fiyat\": r.get(\"fiyat\"),\n",
    "                \"marka\": r.get(\"marka\"),\n",
    "                \"seri\": r.get(\"seri\"),\n",
    "                \"model\": r.get(\"model\"),\n",
    "                \"yil\": r.get(\"yil_num\") or r.get(\"yil\"),\n",
    "                \"kilometre\": r.get(\"km_num\") or r.get(\"kilometre\"),\n",
    "                \"yakit_tipi\": r.get(\"yakit_tipi\"),\n",
    "                \"vites_tipi\": r.get(\"vites_tipi\"),\n",
    "                \"kasa_tipi\": r.get(\"kasa_tipi\"),\n",
    "                \"arac_durumu\": r.get(\"arac_durumu\"),\n",
    "                \"cekis\": r.get(\"cekis\"),\n",
    "                \"motor_hacmi\": r.get(\"motor_hacmi\"),\n",
    "                \"motor_gucu\": r.get(\"motor_gucu\"),\n",
    "                \"tramer\": r.get(\"tramer\"),\n",
    "                \"url\": r.get(\"url\"),\n",
    "\n",
    "                # sorguda filtre için sayısal/küçük harf anahtarlar\n",
    "                \"fiyat_num\": r.get(\"fiyat_num\"),\n",
    "                \"km_num\": r.get(\"km_num\"),\n",
    "                \"yil_num\": r.get(\"yil_num\"),\n",
    "                \"marka_key\": ascii_lower(r.get(\"marka\")),\n",
    "                \"seri_key\": ascii_lower(r.get(\"seri\")),\n",
    "                \"model_key\": ascii_lower(r.get(\"model\")),\n",
    "                \"konum_key\": ascii_lower(r.get(\"konum\")),\n",
    "\n",
    "                # arama metni (dense + sparse için)\n",
    "                \"text\": t,\n",
    "            }\n",
    "            points.append(PointStruct(id=pid, vector=v, payload=payload))\n",
    "\n",
    "        client.upsert(collection_name=collection, points=points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8032052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QueryFilters:\n",
    "    marka: Optional[str] = None\n",
    "    seri: Optional[str] = None\n",
    "    model: Optional[str] = None\n",
    "    konum: Optional[str] = None\n",
    "    fiyat_max: Optional[float] = None\n",
    "    fiyat_min: Optional[float] = None\n",
    "    yil_min: Optional[int] = None\n",
    "    yil_max: Optional[int] = None\n",
    "\n",
    "def build_qdrant_filter(f: QueryFilters) -> Filter | None:\n",
    "    must: list = []\n",
    "\n",
    "    def eq(field: str, val: str):\n",
    "        val = (val or \"\").strip().lower()\n",
    "        if val:\n",
    "            must.append(FieldCondition(key=field, match=MatchValue(value=val)))\n",
    "\n",
    "    def rng(field: str, gte=None, lte=None):\n",
    "        cond = {}\n",
    "        if gte is not None: cond[\"gte\"] = float(gte)\n",
    "        if lte is not None: cond[\"lte\"] = float(lte)\n",
    "        if cond:\n",
    "            must.append(FieldCondition(key=field, range=Range(**cond)))\n",
    "\n",
    "    eq(\"marka_key\", f.marka)\n",
    "    eq(\"seri_key\",  f.seri)\n",
    "    eq(\"model_key\", f.model)\n",
    "    eq(\"konum_key\", f.konum)\n",
    "\n",
    "    rng(\"fiyat_num\", gte=f.fiyat_min, lte=f.fiyat_max)\n",
    "    rng(\"yil_num\",   gte=f.yil_min,   lte=f.yil_max)\n",
    "\n",
    "    return Filter(must=must) if must else None\n",
    "\n",
    "def _is_valid_text(s: str, min_len: int = 3) -> bool:\n",
    "    return isinstance(s, str) and len(s.strip()) >= min_len\n",
    "\n",
    "class HybridSearcher:\n",
    "    def __init__(self, client, collection: str, embedder):\n",
    "        self.client = client\n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "        self._tfidf: TfidfVectorizer | None = None\n",
    "        self._sparse = None\n",
    "        self._ids: np.ndarray | None = None\n",
    "        self._texts: List[str] | None = None\n",
    "\n",
    "    def _ensure_sparse_index(self, max_points: int = 20000):\n",
    "        if self._tfidf is not None:\n",
    "            return\n",
    "\n",
    "        texts: List[str] = []\n",
    "        ids:   List[str] = []\n",
    "\n",
    "        # Qdrant scroll ile sayfalama\n",
    "        next_offset = None\n",
    "        seen = set()\n",
    "        while True:\n",
    "            recs, next_offset = self.client.scroll(\n",
    "                collection_name=self.collection,\n",
    "                with_payload=True,\n",
    "                limit=1024,\n",
    "                offset=next_offset\n",
    "            )\n",
    "\n",
    "            if not recs:\n",
    "                break\n",
    "\n",
    "            for p in recs:\n",
    "                pid = str(p.id)\n",
    "                if pid in seen:\n",
    "                    continue\n",
    "                pl = p.payload or {}\n",
    "                t = pl.get(\"text\", \"\")\n",
    "                if _is_valid_text(t):\n",
    "                    texts.append(t)\n",
    "                    ids.append(pid)\n",
    "                    seen.add(pid)\n",
    "\n",
    "            if next_offset is None or len(texts) >= max_points:\n",
    "                break\n",
    "\n",
    "        # boş kalırsa TF-IDF fit patlamasın\n",
    "        if not texts:\n",
    "            # Minimum bir dummy belge ekleyerek fit'i güvene al\n",
    "            texts = [\"placeholder\"]\n",
    "            ids   = [\"__placeholder__\"]\n",
    "\n",
    "        self._tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "        self._sparse = self._tfidf.fit_transform(texts)\n",
    "        self._ids = np.array(ids)\n",
    "        self._texts = texts\n",
    "\n",
    "    def dense_search(self, query: str, f: Optional[QueryFilters], top_k: int = 50):\n",
    "        qv = self.embedder.embed_query(query)\n",
    "        qf = build_qdrant_filter(f) if f else None\n",
    "        res = self.client.search(\n",
    "            collection_name=self.collection,\n",
    "            query_vector=qv,\n",
    "            query_filter=qf,\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "            with_vectors=False\n",
    "        )\n",
    "        return [(str(r.id), float(r.score), r.payload) for r in res]\n",
    "\n",
    "    def sparse_search(self, query: str, f: Optional[QueryFilters], top_k: int = 200):\n",
    "        self._ensure_sparse_index()\n",
    "        q = self._tfidf.transform([query])                 # csr_matrix (1 x V)\n",
    "        # TF-IDF default: l2 normalize; dot product ≈ cosine\n",
    "        sim = (q @ self._sparse.T).toarray().ravel()       # ndarray (N,)\n",
    "\n",
    "        # En hızlı top-k (tam sıralama yerine)\n",
    "        k = min(top_k, sim.size)\n",
    "        if k == 0:\n",
    "            return []\n",
    "        idx = np.argpartition(-sim, k-1)[:k]               # ilk k skorun indeksleri (sırasız)\n",
    "        order = idx[np.argsort(-sim[idx])]                 # skora göre sırala\n",
    "\n",
    "        results = []\n",
    "        for i in order:\n",
    "            pid = self._ids[i]\n",
    "            sc  = float(sim[i])\n",
    "\n",
    "            # Point + payload çek\n",
    "            pts = self.client.retrieve(self.collection, ids=[pid], with_payload=True)\n",
    "            if not pts:\n",
    "                continue\n",
    "            pl = pts[0].payload or {}\n",
    "\n",
    "            # Payload filtrelerini uygula (Qdrant filter sparse'ta kullanılmadı)\n",
    "            if f:\n",
    "                if f.marka and (pl.get(\"marka_key\") or \"\") != (f.marka or \"\").strip().lower():  continue\n",
    "                if f.seri  and (pl.get(\"seri_key\")  or \"\") != (f.seri  or \"\").strip().lower():  continue\n",
    "                if f.model and (pl.get(\"model_key\") or \"\") != (f.model or \"\").strip().lower():  continue\n",
    "                if f.konum and (pl.get(\"konum_key\") or \"\") != (f.konum or \"\").strip().lower():  continue\n",
    "                if f.fiyat_min is not None and (pl.get(\"fiyat_num\") is None or pl[\"fiyat_num\"] < f.fiyat_min): continue\n",
    "                if f.fiyat_max is not None and (pl.get(\"fiyat_num\") is None or pl[\"fiyat_num\"] > f.fiyat_max): continue\n",
    "                if f.yil_min  is not None and (pl.get(\"yil_num\")   is None or pl[\"yil_num\"]   < f.yil_min):  continue\n",
    "                if f.yil_max  is not None and (pl.get(\"yil_num\")   is None or pl[\"yil_num\"]   > f.yil_max):  continue\n",
    "\n",
    "            results.append((pid, sc, pl))\n",
    "        return results\n",
    "\n",
    "\n",
    "    def rrf_merge(self, dense, sparse, k: float = 60.0, top_k: int = 50):\n",
    "        # dense/sparse: [(id, score, payload)]\n",
    "        # RRF için önce id->rank\n",
    "        def ranks(lst):\n",
    "            return {pid: rank for rank, (pid, _, _) in enumerate(sorted(lst, key=lambda x: -x[1]), start=1)}\n",
    "        rd = ranks(dense)\n",
    "        rs = ranks(sparse)\n",
    "        ids = set([pid for pid,_,_ in dense] + [pid for pid,_,_ in sparse])\n",
    "        merged = []\n",
    "        for pid in ids:\n",
    "            r1 = rd.get(pid, 10**6)\n",
    "            r2 = rs.get(pid, 10**6)\n",
    "            rrf = 1.0/(k+r1) + 1.0/(k+r2)\n",
    "            payload = None\n",
    "            # payload çek\n",
    "            if pid in rd:\n",
    "                payload = [pl for (p,_,pl) in dense if p==pid][0]\n",
    "            elif pid in rs:\n",
    "                payload = [pl for (p,_,pl) in sparse if p==pid][0]\n",
    "            merged.append((pid, rrf, payload))\n",
    "        merged.sort(key=lambda x: -x[1])\n",
    "        return merged[:top_k]\n",
    "\n",
    "    def search(self, query_text: str, f: Optional[QueryFilters] = None, top_k: int = 30):\n",
    "        dense = self.dense_search(query_text, f, top_k=top_k)\n",
    "        sparse = self.sparse_search(query_text, f, top_k=top_k*4)\n",
    "        merged = self.rrf_merge(dense, sparse, top_k=top_k)\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d54b2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST_Embedder:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "                 force_device: str = \"cpu\"):  # \"cpu\" veya \"cuda\"\n",
    "        self.device = force_device\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)  # CPU'ya zorladık\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        try:\n",
    "            return self.model.encode(\n",
    "                texts,\n",
    "                convert_to_numpy=True,\n",
    "                batch_size=64,\n",
    "                normalize_embeddings=True,   # cosine için iyi pratik\n",
    "                show_progress_bar=False\n",
    "            ).tolist()\n",
    "        except Exception:\n",
    "            # GPU'da hata olursa otomatik CPU'ya düş\n",
    "            if self.device != \"cpu\":\n",
    "                self.device = \"cpu\"\n",
    "                self.model = SentenceTransformer(self.model._model_card, device=\"cpu\")\n",
    "                arr = self.model.encode(\n",
    "                    texts, convert_to_numpy=True, batch_size=64, normalize_embeddings=True, show_progress_bar=False\n",
    "                ).tolist()\n",
    "                return arr\n",
    "            raise\n",
    "\n",
    "    def embed_query(self, text: str):\n",
    "        vec = self.embed_documents([text])[0]\n",
    "        return vec\n",
    "\n",
    "    def dimension(self) -> int:\n",
    "        # SBERT modelleri bu özelliği sağlar; yoksa örnekle öğren\n",
    "        if hasattr(self.model, \"get_sentence_embedding_dimension\"):\n",
    "            return self.model.get_sentence_embedding_dimension()\n",
    "        return len(self.embed_query(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5551a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 0) Qdrant bağlantısı\\nclient = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\\n\\n# 1) DataFrame yükle ve normalize et\\ndf = pd.read_parquet(\"ilanlar.parquet\")   # veya csv\\ndf = normalize_df(df)\\n\\n# 2) Embedder (CPU)\\nembedder = ST_Embedder(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", force_device=\"cpu\")\\n\\n# 3) Upsert\\ncollection = \"car_listings_st\"   # 384-dim modeller için ayrı koleksiyon önerilir\\ndf_to_points(df, embedder, collection, client, batch_size=256)\\n\\n# 4) Arama\\nsearcher = HybridSearcher(client, collection, embedder)\\nuser_query = \"İstanbul’da 1.3 milyon TL’ye kadar, 2018 üzeri otomatik benzinli Astra\"\\nfilters = heuristic_parse(user_query)  # veya LLM tabanlı parser\\nresults = searcher.search(user_query, filters, top_k=20)\\n\\n# results: [(id, score, payload), ...]\\n# payload[\\'marka\\'], payload[\\'model\\'], payload[\\'yil_num\\'], payload[\\'fiyat_num\\'], payload[\\'url\\'] ...\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# car_vector_search.py\n",
    "# Gereksinimler:\n",
    "# pip install pandas qdrant-client sentence-transformers scikit-learn unidecode pydantic tqdm\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    VectorParams,\n",
    "    PointStruct,\n",
    "    Filter,\n",
    "    FieldCondition,\n",
    "    Range,\n",
    "    MatchValue,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Embedder (Sentence-Transformers - CPU varsayılan)\n",
    "# -----------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class ST_Embedder:\n",
    "    \"\"\"\n",
    "    Sentence-Transformers tabanlı embedder.\n",
    "    - Varsayılan cihaz: CPU\n",
    "    - normalize_embeddings=True (cosine için iyi pratik)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "                 force_device: str = \"cpu\"):\n",
    "        self.device = force_device\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        try:\n",
    "            return self.model.encode(\n",
    "                texts,\n",
    "                convert_to_numpy=True,\n",
    "                batch_size=64,\n",
    "                normalize_embeddings=True,\n",
    "                show_progress_bar=False,\n",
    "            ).tolist()\n",
    "        except Exception:\n",
    "            # GPU'da sorun olursa CPU'ya düş\n",
    "            if self.device != \"cpu\":\n",
    "                self.device = \"cpu\"\n",
    "                self.model = SentenceTransformer(self.model_name, device=\"cpu\")\n",
    "                return self.model.encode(\n",
    "                    texts,\n",
    "                    convert_to_numpy=True,\n",
    "                    batch_size=64,\n",
    "                    normalize_embeddings=True,\n",
    "                    show_progress_bar=False,\n",
    "                ).tolist()\n",
    "            raise\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "    def dimension(self) -> int:\n",
    "        if hasattr(self.model, \"get_sentence_embedding_dimension\"):\n",
    "            return self.model.get_sentence_embedding_dimension()\n",
    "        # çok nadir durumda:\n",
    "        return len(self.embed_query(\"test\"))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Yardımcılar / Normalizasyon\n",
    "# -----------------------------\n",
    "TURKISH_MAP = {\n",
    "    \"otomatik\": [\"otomatik\", \"auto\", \"dct\", \"edc\", \"e-cvt\", \"cvt\", \"tiptronic\", \"multitronic\", \"dsg\"],\n",
    "    \"manuel\": [\"manuel\", \"manual\"],\n",
    "    \"benzin\": [\"benzin\", \"gasoline\", \"benzinli\"],\n",
    "    \"dizel\": [\"dizel\", \"diesel\"],\n",
    "    \"lpg\": [\"lpg\", \"autogas\"],\n",
    "    \"hybrid\": [\"hibrid\", \"hybrid\"],\n",
    "    \"elektrik\": [\"elektrik\", \"electric\", \"bev\", \"ev\"],\n",
    "}\n",
    "\n",
    "def ascii_lower(s: Any) -> str:\n",
    "    return unidecode(str(s or \"\")).strip().lower()\n",
    "\n",
    "def to_num(text: Any) -> Optional[float]:\n",
    "    if text is None:\n",
    "        return None\n",
    "    s = str(text).strip().lower()\n",
    "    if s in (\"nan\", \"\", \"none\", \"yok\", \"—\", \"-\"):\n",
    "        return None\n",
    "    s = s.replace(\"tl\", \"\").replace(\"₺\", \"\").replace(\"km\", \"\")\n",
    "    s = s.replace(\"milyon\", \"000000\").replace(\"mn\", \"000000\").replace(\"m\", \"000000\")\n",
    "    s = re.sub(\n",
    "        r\"(\\d+)[\\.,]?(\\d*)\\s*bin\",\n",
    "        lambda m: str(float(m.group(1) + \".\" + (m.group(2) or \"0\")) * 1000),\n",
    "        s,\n",
    "    )\n",
    "    s = s.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    s = s.replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def year4(x: Any) -> Optional[int]:\n",
    "    m = re.search(r\"\\b(19|20)\\d{2}\\b\", str(x or \"\"))\n",
    "    return int(m.group(0)) if m else None\n",
    "\n",
    "def none_if_nan(x: Any):\n",
    "    try:\n",
    "        return None if (x is None or (isinstance(x, float) and math.isnan(x))) else x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def make_point_id(raw: Any):\n",
    "    \"\"\"\n",
    "    Qdrant ID: unsigned int veya UUID string olmalı.\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return str(uuid.uuid4())\n",
    "    # int?\n",
    "    try:\n",
    "        if isinstance(raw, float):\n",
    "            if math.isnan(raw):\n",
    "                return str(uuid.uuid4())\n",
    "            if float(raw).is_integer() and int(raw) >= 0:\n",
    "                return int(raw)\n",
    "            return str(uuid.uuid4())\n",
    "        iv = int(raw)\n",
    "        if iv >= 0:\n",
    "            return iv\n",
    "    except Exception:\n",
    "        pass\n",
    "    # UUID?\n",
    "    try:\n",
    "        return str(uuid.UUID(str(raw)))\n",
    "    except Exception:\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "def match_from_map(value: str, mapping: Dict[str, List[str]]) -> str:\n",
    "    v = ascii_lower(value)\n",
    "    for canon, variants in mapping.items():\n",
    "        for t in variants:\n",
    "            if t in v:\n",
    "                return canon\n",
    "    return value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) DataFrame Normalize\n",
    "# -----------------------------\n",
    "def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    expected = [\n",
    "        \"id\",\"baslik\",\"konum\",\"fiyat\",\"aciklama\",\"marka\",\"seri\",\"model\",\"yil\",\"kilometre\",\n",
    "        \"yakit_tipi\",\"vites_tipi\",\"renk\",\"arac_durumu\",\"kasa_tipi\",\"cekis\",\"motor_hacmi\",\n",
    "        \"motor_gucu\",\"tramer\",\"url\"\n",
    "    ]\n",
    "    missing = [c for c in expected if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Eksik kolon(lar): {missing}\")\n",
    "\n",
    "    # metin\n",
    "    for col in [\n",
    "        \"baslik\",\"konum\",\"aciklama\",\"marka\",\"seri\",\"model\",\"yakit_tipi\",\"vites_tipi\",\n",
    "        \"renk\",\"arac_durumu\",\"kasa_tipi\",\"cekis\",\"url\"\n",
    "    ]:\n",
    "        df[col] = df[col].map(lambda x: re.sub(r\"\\s+\", \" \", str(x or \"\").strip()))\n",
    "\n",
    "    # sayısal\n",
    "    df[\"fiyat_num\"] = df[\"fiyat\"].map(to_num)\n",
    "    df[\"km_num\"] = df[\"kilometre\"].map(to_num)\n",
    "    df[\"tramer_num\"] = df[\"tramer\"].map(to_num)\n",
    "    df[\"yil_num\"] = df[\"yil\"].map(year4)\n",
    "\n",
    "    # vites / yakıt std\n",
    "    df[\"vites_std\"] = df[\"vites_tipi\"].apply(\n",
    "        lambda v: match_from_map(v, {\"otomatik\": TURKISH_MAP[\"otomatik\"], \"manuel\": TURKISH_MAP[\"manuel\"]})\n",
    "    )\n",
    "    def yakit_std(v):\n",
    "        if match_from_map(v, {\"benzin\": TURKISH_MAP[\"benzin\"]}) == \"benzin\": return \"benzin\"\n",
    "        if match_from_map(v, {\"dizel\": TURKISH_MAP[\"dizel\"]}) == \"dizel\": return \"dizel\"\n",
    "        if match_from_map(v, {\"lpg\": TURKISH_MAP[\"lpg\"]}) == \"lpg\": return \"lpg\"\n",
    "        if match_from_map(v, {\"hybrid\": TURKISH_MAP[\"hybrid\"]}) == \"hybrid\": return \"hybrid\"\n",
    "        if match_from_map(v, {\"elektrik\": TURKISH_MAP[\"elektrik\"]}) == \"elektrik\": return \"elektrik\"\n",
    "        return ascii_lower(v)\n",
    "    df[\"yakit_std\"] = df[\"yakit_tipi\"].map(yakit_std)\n",
    "\n",
    "    # arama anahtarları\n",
    "    for col in [\"marka\",\"seri\",\"model\",\"konum\",\"kasa_tipi\",\"cekis\",\"renk\",\"arac_durumu\"]:\n",
    "        df[col + \"_key\"] = df[col].map(ascii_lower)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Doc metni + payload\n",
    "# -----------------------------\n",
    "def build_doc_text(row: Dict[str, Any]) -> str:\n",
    "    marka = str(row.get(\"marka\", \"\")).strip()\n",
    "    seri = str(row.get(\"seri\", \"\")).strip()\n",
    "    model = str(row.get(\"model\", \"\")).strip()\n",
    "    yil = row.get(\"yil_num\") or row.get(\"yil\") or \"\"\n",
    "    vites = row.get(\"vites_tipi\", \"\")\n",
    "    yakit = row.get(\"yakit_tipi\", \"\")\n",
    "    km = row.get(\"kilometre\", \"\")\n",
    "    konum = row.get(\"konum\", \"\")\n",
    "    kasa = row.get(\"kasa_tipi\", \"\")\n",
    "    fiyat = row.get(\"fiyat\", \"\")\n",
    "    aciklama = (row.get(\"aciklama\") or \"\").strip()\n",
    "\n",
    "    title = f\"{marka} {seri} {model} {yil}\".strip()\n",
    "    bullet = f\"{yakit}, {vites}, {km} km, {kasa}, {konum}\".replace(\"  \",\" \").strip(\" ,\")\n",
    "    text = f\"{title} – {bullet}. Fiyat: {fiyat}. {aciklama}\"\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def build_payload(r: Dict[str, Any], text: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        # ham alanlar\n",
    "        \"id\": r.get(\"id\"),\n",
    "        \"baslik\": r.get(\"baslik\"),\n",
    "        \"konum\": r.get(\"konum\"),\n",
    "        \"fiyat\": r.get(\"fiyat\"),\n",
    "        \"marka\": r.get(\"marka\"),\n",
    "        \"seri\": r.get(\"seri\"),\n",
    "        \"model\": r.get(\"model\"),\n",
    "        \"yil\": r.get(\"yil_num\") or r.get(\"yil\"),\n",
    "        \"kilometre\": r.get(\"km_num\") or r.get(\"kilometre\"),\n",
    "        \"yakit_tipi\": r.get(\"yakit_tipi\"),\n",
    "        \"vites_tipi\": r.get(\"vites_tipi\"),\n",
    "        \"renk\": r.get(\"renk\"),\n",
    "        \"arac_durumu\": r.get(\"arac_durumu\"),\n",
    "        \"kasa_tipi\": r.get(\"kasa_tipi\"),\n",
    "        \"cekis\": r.get(\"cekis\"),\n",
    "        \"motor_hacmi\": r.get(\"motor_hacmi\"),\n",
    "        \"motor_gucu\": r.get(\"motor_gucu\"),\n",
    "        \"tramer\": r.get(\"tramer\"),\n",
    "        \"url\": r.get(\"url\"),\n",
    "\n",
    "        # sayısal & key alanlar\n",
    "        \"fiyat_num\": none_if_nan(r.get(\"fiyat_num\")),\n",
    "        \"km_num\": none_if_nan(r.get(\"km_num\")),\n",
    "        \"yil_num\": none_if_nan(r.get(\"yil_num\")),\n",
    "        \"marka_key\": ascii_lower(r.get(\"marka\")),\n",
    "        \"seri_key\": ascii_lower(r.get(\"seri\")),\n",
    "        \"model_key\": ascii_lower(r.get(\"model\")),\n",
    "        \"konum_key\": ascii_lower(r.get(\"konum\")),\n",
    "        \"kasa_tipi_key\": ascii_lower(r.get(\"kasa_tipi\")),\n",
    "        \"cekis_key\": ascii_lower(r.get(\"cekis\")),\n",
    "        \"renk_key\": ascii_lower(r.get(\"renk\")),\n",
    "        \"arac_durumu_key\": ascii_lower(r.get(\"arac_durumu\")),\n",
    "\n",
    "        # arama metni\n",
    "        \"text\": text,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Qdrant collection garanti\n",
    "# -----------------------------\n",
    "def ensure_collection(client: QdrantClient, collection: str, dim: int, distance: Distance = Distance.COSINE):\n",
    "    existing = [c.name for c in client.get_collections().collections]\n",
    "    if collection in existing:\n",
    "        # Boyut uyuşmasını kontrol et (Qdrant sürümüne göre alanlar değişebilir)\n",
    "        info = client.get_collection(collection)\n",
    "        # Bazı sürümlerde: info.config.params.vectors.size\n",
    "        current_dim = None\n",
    "        try:\n",
    "            current_dim = info.config.params.vectors.size  # type: ignore\n",
    "        except Exception:\n",
    "            # Yedek yol: vektör sayısı boyut değil, o yüzden kullanma\n",
    "            pass\n",
    "        if current_dim is not None and current_dim != dim:\n",
    "            raise ValueError(f\"Koleksiyon '{collection}' farklı boyutta: {current_dim} ≠ {dim}\")\n",
    "        return\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection,\n",
    "        vectors_config=VectorParams(size=dim, distance=distance),\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) DF → Qdrant Upsert\n",
    "# -----------------------------\n",
    "def df_to_points(df: pd.DataFrame, embedder: ST_Embedder, collection: str,\n",
    "                 client: QdrantClient, batch_size: int = 256):\n",
    "    if \"fiyat_num\" not in df.columns:\n",
    "        df[\"fiyat_num\"] = df[\"fiyat\"].map(to_num)\n",
    "    if \"km_num\" not in df.columns:\n",
    "        df[\"km_num\"] = df[\"kilometre\"].map(to_num)\n",
    "    if \"yil_num\" not in df.columns:\n",
    "        df[\"yil_num\"] = df[\"yil\"].map(year4)\n",
    "\n",
    "    dim = embedder.dimension()\n",
    "    ensure_collection(client, collection, dim)\n",
    "\n",
    "    rows = df.to_dict(orient=\"records\")\n",
    "    for i in tqdm(range(0, len(rows), batch_size), desc=\"upserting\"):\n",
    "        chunk = rows[i:i + batch_size]\n",
    "        texts = [build_doc_text(r) for r in chunk]\n",
    "        vecs = embedder.embed_documents(texts)\n",
    "\n",
    "        points = []\n",
    "        for r, v, t in zip(chunk, vecs, texts):\n",
    "            pid = make_point_id(r.get(\"id\"))\n",
    "            payload = build_payload(r, t)\n",
    "            points.append(PointStruct(id=pid, vector=v, payload=payload))\n",
    "\n",
    "        client.upsert(collection_name=collection, points=points)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Filtre modeli (sorgu → payload filter)\n",
    "# -----------------------------\n",
    "class QueryFilters(BaseModel):\n",
    "    marka: Optional[str] = None\n",
    "    seri: Optional[str] = None\n",
    "    model: Optional[str] = None\n",
    "    konum: Optional[str] = None\n",
    "    fiyat_min: Optional[float] = None\n",
    "    fiyat_max: Optional[float] = None\n",
    "    yil_min: Optional[int] = None\n",
    "    yil_max: Optional[int] = None\n",
    "\n",
    "def build_qdrant_filter(f: QueryFilters) -> Optional[Filter]:\n",
    "    must: List[FieldCondition] = []\n",
    "\n",
    "    def eq(field: str, val: Optional[str]):\n",
    "        val = (val or \"\").strip().lower()\n",
    "        if val:\n",
    "            must.append(FieldCondition(key=field, match=MatchValue(value=val)))\n",
    "\n",
    "    def rng(field: str, gte=None, lte=None):\n",
    "        cond = {}\n",
    "        if gte is not None: cond[\"gte\"] = float(gte)\n",
    "        if lte is not None: cond[\"lte\"] = float(lte)\n",
    "        if cond:\n",
    "            must.append(FieldCondition(key=field, range=Range(**cond)))\n",
    "\n",
    "    eq(\"marka_key\", f.marka)\n",
    "    eq(\"seri_key\",  f.seri)\n",
    "    eq(\"model_key\", f.model)\n",
    "    eq(\"konum_key\", f.konum)\n",
    "    rng(\"fiyat_num\", gte=f.fiyat_min, lte=f.fiyat_max)\n",
    "    rng(\"yil_num\",   gte=f.yil_min,   lte=f.yil_max)\n",
    "\n",
    "    return Filter(must=must) if must else None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) HybridSearcher (Dense + Sparse TF-IDF + RRF)\n",
    "# -----------------------------\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def _is_valid_text(s: str, min_len: int = 3) -> bool:\n",
    "    return isinstance(s, str) and len(s.strip()) >= min_len\n",
    "\n",
    "class HybridSearcher:\n",
    "    def __init__(self, client: QdrantClient, collection: str, embedder: ST_Embedder):\n",
    "        self.client = client\n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "        self._tfidf: Optional[TfidfVectorizer] = None\n",
    "        self._sparse = None\n",
    "        self._ids: Optional[np.ndarray] = None   # dtype=object\n",
    "        self._texts: Optional[List[str]] = None\n",
    "        self._payloads: Dict[Any, Dict] = {}     # id -> payload (cache)\n",
    "\n",
    "    def _ensure_sparse_index(self, max_points: int = 20000):\n",
    "        if self._tfidf is not None:\n",
    "            return\n",
    "\n",
    "        texts: List[str] = []\n",
    "        ids:   List[Any] = []\n",
    "        seen = set()\n",
    "        next_offset = None\n",
    "\n",
    "        while True:\n",
    "            recs, next_offset = self.client.scroll(\n",
    "                collection_name=self.collection,\n",
    "                with_payload=True,\n",
    "                limit=1024,\n",
    "                offset=next_offset,\n",
    "            )\n",
    "            if not recs:\n",
    "                break\n",
    "\n",
    "            for p in recs:\n",
    "                pid = p.id  # tipini KORU (int ya da str-UUID)\n",
    "                if pid in seen:\n",
    "                    continue\n",
    "                pl = p.payload or {}\n",
    "                t = pl.get(\"text\", \"\")\n",
    "                if _is_valid_text(t):\n",
    "                    texts.append(t)\n",
    "                    ids.append(pid)\n",
    "                    self._payloads[pid] = pl\n",
    "                    seen.add(pid)\n",
    "\n",
    "            if next_offset is None or len(texts) >= max_points:\n",
    "                break\n",
    "\n",
    "        if not texts:\n",
    "            texts = [\"placeholder\"]\n",
    "            ids = [\"__placeholder__\"]\n",
    "            self._payloads[\"__placeholder__\"] = {\"text\": \"placeholder\"}\n",
    "\n",
    "        self._tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "        self._sparse = self._tfidf.fit_transform(texts)\n",
    "        self._ids = np.array(ids, dtype=object)\n",
    "        self._texts = texts\n",
    "\n",
    "    def dense_search(self, query: str, f: Optional[QueryFilters], top_k: int = 50):\n",
    "        qv = self.embedder.embed_query(query)\n",
    "        qf = build_qdrant_filter(f) if f else None\n",
    "        res = self.client.search(\n",
    "            collection_name=self.collection,\n",
    "            query_vector=qv,\n",
    "            query_filter=qf,\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "        )\n",
    "        return [(r.id, float(r.score), r.payload or {}) for r in res]\n",
    "\n",
    "    def sparse_search(self, query: str, f: Optional[QueryFilters], top_k: int = 200):\n",
    "        self._ensure_sparse_index()\n",
    "        q = self._tfidf.transform([query])\n",
    "        sim = (q @ self._sparse.T).toarray().ravel()\n",
    "\n",
    "        k = min(top_k, sim.size)\n",
    "        if k == 0:\n",
    "            return []\n",
    "        idx = np.argpartition(-sim, k - 1)[:k]\n",
    "        order = idx[np.argsort(-sim[idx])]\n",
    "\n",
    "        results = []\n",
    "        for i in order:\n",
    "            pid = self._ids[i]\n",
    "            sc = float(sim[i])\n",
    "            pl = self._payloads.get(pid, {})\n",
    "\n",
    "            # Payload filtreleri\n",
    "            if f:\n",
    "                if f.marka and (pl.get(\"marka_key\") or \"\") != (f.marka or \"\").strip().lower():  continue\n",
    "                if f.seri  and (pl.get(\"seri_key\")  or \"\") != (f.seri  or \"\").strip().lower():  continue\n",
    "                if f.model and (pl.get(\"model_key\") or \"\") != (f.model or \"\").strip().lower():  continue\n",
    "                if f.konum and (pl.get(\"konum_key\") or \"\") != (f.konum or \"\").strip().lower():  continue\n",
    "                if f.fiyat_min is not None and (pl.get(\"fiyat_num\") is None or pl[\"fiyat_num\"] < f.fiyat_min): continue\n",
    "                if f.fiyat_max is not None and (pl.get(\"fiyat_num\") is None or pl[\"fiyat_num\"] > f.fiyat_max): continue\n",
    "                if f.yil_min  is not None and (pl.get(\"yil_num\")   is None or pl[\"yil_num\"]   < f.yil_min):  continue\n",
    "                if f.yil_max  is not None and (pl.get(\"yil_num\")   is None or pl[\"yil_num\"]   > f.yil_max):  continue\n",
    "\n",
    "            results.append((pid, sc, pl))\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def rrf_merge(dense: List[Tuple[Any, float, Dict]], sparse: List[Tuple[Any, float, Dict]],\n",
    "                  k: float = 60.0, top_k: int = 50):\n",
    "        def ranks(lst):\n",
    "            return {pid: rank for rank, (pid, _, _) in enumerate(sorted(lst, key=lambda x: -x[1]), start=1)}\n",
    "        rd = ranks(dense)\n",
    "        rs = ranks(sparse)\n",
    "        ids = set([pid for pid, _, _ in dense] + [pid for pid, _, _ in sparse])\n",
    "        merged = []\n",
    "        for pid in ids:\n",
    "            r1 = rd.get(pid, 10**6)\n",
    "            r2 = rs.get(pid, 10**6)\n",
    "            rrf = 1.0 / (k + r1) + 1.0 / (k + r2)\n",
    "            payload = None\n",
    "            if pid in rd:\n",
    "                payload = [pl for (p, _, pl) in dense if p == pid][0]\n",
    "            elif pid in rs:\n",
    "                payload = [pl for (p, _, pl) in sparse if p == pid][0]\n",
    "            merged.append((pid, rrf, payload or {}))\n",
    "        merged.sort(key=lambda x: -x[1])\n",
    "        return merged[:top_k]\n",
    "\n",
    "    def search(self, query_text: str, f: Optional[QueryFilters] = None, top_k: int = 30):\n",
    "        dense = self.dense_search(query_text, f, top_k=top_k)\n",
    "        sparse = self.sparse_search(query_text, f, top_k=top_k * 4)\n",
    "        return self.rrf_merge(dense, sparse, top_k=top_k)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Örnek kullanım (çalıştırmayın)\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "# 0) Qdrant bağlantısı\n",
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\n",
    "\n",
    "# 1) DataFrame yükle ve normalize et\n",
    "df = pd.read_parquet(\"ilanlar.parquet\")   # veya csv\n",
    "df = normalize_df(df)\n",
    "\n",
    "# 2) Embedder (CPU)\n",
    "embedder = ST_Embedder(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", force_device=\"cpu\")\n",
    "\n",
    "# 3) Upsert\n",
    "collection = \"car_listings_st\"   # 384-dim modeller için ayrı koleksiyon önerilir\n",
    "df_to_points(df, embedder, collection, client, batch_size=256)\n",
    "\n",
    "# 4) Arama\n",
    "searcher = HybridSearcher(client, collection, embedder)\n",
    "user_query = \"İstanbul’da 1.3 milyon TL’ye kadar, 2018 üzeri otomatik benzinli Astra\"\n",
    "filters = heuristic_parse(user_query)  # veya LLM tabanlı parser\n",
    "results = searcher.search(user_query, filters, top_k=20)\n",
    "\n",
    "# results: [(id, score, payload), ...]\n",
    "# payload['marka'], payload['model'], payload['yil_num'], payload['fiyat_num'], payload['url'] ...\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00dbf848",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02139a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ST_Embedder(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", force_device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0ae47db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "upserting: 100%|██████████| 258/258 [12:47<00:00,  2.98s/it]\n"
     ]
    }
   ],
   "source": [
    "collection = \"car_listings_st\"   # 384-dim modeller için ayrı koleksiyon önerilir\n",
    "df_to_points(df, embedder, collection, client, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# LLM tabanlı filtre çıkarma - DÜZELTİLMİŞ VERSİYON\n",
    "# ============================\n",
    "from typing import Optional, List, Dict, Any, Union\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from qdrant_client.models import Filter as QFilter, FieldCondition, MatchValue, Range\n",
    "\n",
    "class RangeSpec(BaseModel):\n",
    "    gte: Optional[float] = None\n",
    "    lte: Optional[float] = None\n",
    "\n",
    "class FilterSpec(BaseModel):\n",
    "    must: Dict[str, Union[str, List[str]]] = Field(default_factory=dict)\n",
    "    must_not: Dict[str, Union[str, List[str]]] = Field(default_factory=dict)\n",
    "    should: Dict[str, List[str]] = Field(default_factory=dict)\n",
    "    ranges: Dict[str, RangeSpec] = Field(default_factory=dict)\n",
    "\n",
    "# Alan adları eşlemesi (LLM yanlış anahtar verirse düzeltmek için)\n",
    "FIELD_MAP = {\n",
    "    # metin-eşitlik anahtarları\n",
    "    \"marka\": \"marka_key\",\n",
    "    \"seri\": \"seri_key\",\n",
    "    \"model\": \"model_key\",\n",
    "    \"konum\": \"konum_key\",\n",
    "    \"kasa\": \"kasa_tipi_key\",\n",
    "    \"kasa_tipi\": \"kasa_tipi_key\",\n",
    "    \"cekis\": \"cekis_key\",\n",
    "    \"renk\": \"renk_key\",\n",
    "    \"durum\": \"arac_durumu_key\",\n",
    "    \"arac_durumu\": \"arac_durumu_key\",\n",
    "    \"vites\": \"vites_std\",\n",
    "    \"yakit\": \"yakit_std\",\n",
    "\n",
    "    # sayısal/aritmetik anahtarlar\n",
    "    \"fiyat\": \"fiyat_num\",\n",
    "    \"fiyat_num\": \"fiyat_num\",\n",
    "    \"km\": \"km_num\",\n",
    "    \"kilometre\": \"km_num\",\n",
    "    \"km_num\": \"km_num\",\n",
    "    \"yil\": \"yil_num\",\n",
    "    \"yil_num\": \"yil_num\",\n",
    "    \"tramer\": \"tramer_num\",\n",
    "    \"tramer_num\": \"tramer_num\",\n",
    "    \"arac_yasi\": \"Araç_Yası\",\n",
    "    \"araç_yası\": \"Araç_Yası\",\n",
    "}\n",
    "\n",
    "# değer normalizasyonu\n",
    "VALUE_NORMALIZATION = {\n",
    "    \"vites_std\": {\n",
    "        \"otomatik\": [\"otomatik\", \"auto\", \"dct\", \"edc\", \"e-cvt\", \"cvt\", \"tiptronic\", \"multitronic\", \"dsg\"],\n",
    "        \"manuel\":   [\"manuel\", \"manual\"],\n",
    "    },\n",
    "    \"yakit_std\": {\n",
    "        \"benzin\":   [\"benzin\", \"benzinli\", \"gasoline\"],\n",
    "        \"dizel\":    [\"dizel\", \"diesel\"],\n",
    "        \"lpg\":      [\"lpg\", \"autogas\"],\n",
    "        \"hybrid\":   [\"hibrid\", \"hybrid\"],\n",
    "        \"elektrik\": [\"elektrik\", \"electric\", \"bev\", \"ev\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "def _canon_value(field: str, val: str) -> str:\n",
    "    from unidecode import unidecode  # Bu import'u eklemeyi unutmayın\n",
    "    v = unidecode(str(val or \"\")).strip().lower()\n",
    "    mapping = VALUE_NORMALIZATION.get(field)\n",
    "    if not mapping:\n",
    "        return v\n",
    "    for canon, variants in mapping.items():\n",
    "        for t in variants:\n",
    "            if t in v:\n",
    "                return canon\n",
    "    return v\n",
    "\n",
    "# Prompt parçaları - SÜSLü PARANTEZLER ESCAPELENDİ\n",
    "SCHEMA = (\n",
    "    '{{\\n'\n",
    "    '  \"must\": {{ \"<field>\": \"deger\" | [\"deger1\", \"deger2\"] }},\\n'\n",
    "    '  \"must_not\": {{ \"<field>\": \"deger\" | [\"...\"] }},\\n'\n",
    "    '  \"should\": {{ \"<field>\": [\"degerA\",\"degerB\"] }},\\n'\n",
    "    '  \"ranges\": {{ \"<numeric_field>\": {{ \"gte\": <float?>, \"lte\": <float?> }} }}\\n'\n",
    "    '}}\\n'\n",
    ")\n",
    "\n",
    "FEWSHOT = (\n",
    "    '[\\n'\n",
    "    '  {{\\n'\n",
    "    '    \"must\": {{\\n'\n",
    "    '      \"konum_key\": \"istanbul\",\\n'\n",
    "    '      \"marka_key\": \"opel\",\\n'\n",
    "    '      \"seri_key\": \"astra\",\\n'\n",
    "    '      \"vites_std\": \"otomatik\",\\n'\n",
    "    '      \"yakit_std\": \"benzin\"\\n'\n",
    "    '    }},\\n'\n",
    "    '    \"must_not\": {{}},\\n'\n",
    "    '    \"should\": {{}},\\n'\n",
    "    '    \"ranges\": {{\\n'\n",
    "    '      \"fiyat_num\": {{ \"gte\": null, \"lte\": 1300000 }},\\n'\n",
    "    '      \"yil_num\":   {{ \"gte\": 2019, \"lte\": null }}\\n'\n",
    "    '    }}\\n'\n",
    "    '  }},\\n'\n",
    "    '  {{\\n'\n",
    "    '    \"must\": {{ \"yakit_std\": \"dizel\" }},\\n'\n",
    "    '    \"must_not\": {{}},\\n'\n",
    "    '    \"should\": {{ \"konum_key\": [\"ankara\",\"izmir\"] }},\\n'\n",
    "    '    \"ranges\": {{\\n'\n",
    "    '      \"yil_num\": {{ \"gte\": 2016, \"lte\": 2020 }},\\n'\n",
    "    '      \"km_num\":  {{ \"gte\": null, \"lte\": 120000 }}\\n'\n",
    "    '    }}\\n'\n",
    "    '  }}\\n'\n",
    "    ']'\n",
    ")\n",
    "\n",
    "FILTER_SYSTEM = (\n",
    "    \"Sen bir araç arama sorgusu ayrıştırıcısısın.\\n\"\n",
    "    \"Görev: Kullanıcının doğal dildeki arama niyetinden yapısal bir filtre JSON'u çıkar.\\n\\n\"\n",
    "    \"Kurallar:\\n\"\n",
    "    \"- SADECE TEK BİR JSON OBJESI ver. Liste değil, tek obje. Açıklama yazma.\\n\"\n",
    "    \"- Şema aşağıdaki gibidir:\\n\"\n",
    "    \"{SCHEMA}\\n\"\n",
    "    \"- Alan adları payload alanlarına hizalı olmalı (örnekler):\\n\"\n",
    "    \"  marka_key, seri_key, model_key, konum_key, kasa_tipi_key, cekis_key, renk_key, arac_durumu_key, vites_std, yakit_std,\\n\"\n",
    "    \"  fiyat_num, km_num, yil_num, tramer_num, Araç_Yası\\n\"\n",
    "    \"- Eşanlamlıları standardize et: vites_std ∈ {{\\\"otomatik\\\",\\\"manuel\\\"}}; yakit_std ∈ {{\\\"benzin\\\",\\\"dizel\\\",\\\"lpg\\\",\\\"hybrid\\\",\\\"elektrik\\\"}}.\\n\"\n",
    "    \"- \\\"üstü\\\", \\\"altı\\\", \\\"en az\\\", \\\"en çok\\\", \\\"arası\\\", \\\"veya\\\" gibi kalıpları uygun range/list biçimine çevir.\\n\"\n",
    "    \"- Marka/seri/model/konum gibi isimleri *_key alanlarına küçük harf/ASCII normalize et.\\n\"\n",
    "    \"- Belirsizse alanı ekleme. JSON dışına çıkma.\\n\"\n",
    "    \"- ÖNEMLİ: Sadece son sorgu için (3. sorgu) JSON üret, örnekleri dahil etme!\"\n",
    ")\n",
    "\n",
    "FILTER_HUMAN = (\n",
    "    \"İlk iki sorgu örnek, sadece üçüncü sorgu için JSON üret:\\n\\n\"\n",
    "    \"ÖRNEK 1: \\\"İstanbul'da 1.3 milyon TL'ye kadar, 2018 üzeri otomatik benzinli Opel Astra\\\"\\n\"\n",
    "    \"ÖRNEK 2: \\\"Ankara veya İzmir, 2016-2020 arası, km en çok 120 bin, dizel\\\"\\n\\n\"\n",
    "    \"ŞİMDİ BU SORGU İÇİN JSON ÜRET: \\\"{user_query}\\\"\\n\\n\"\n",
    "    \"Örnek çıktı formatı (sadece referans, kopyalama):\\n\"\n",
    "    \"{FEWSHOT}\\n\\n\"\n",
    "    \"SADECE aşağıdaki sorgu için tek bir JSON objesi üret:\\n\"\n",
    "    \"SORGU: \\\"{user_query}\\\"\\n\\n\"\n",
    "    \"{format_instructions}\"\n",
    ")\n",
    "\n",
    "filter_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", FILTER_SYSTEM),\n",
    "    (\"human\", FILTER_HUMAN),\n",
    "]).partial(SCHEMA=SCHEMA, FEWSHOT=FEWSHOT)\n",
    "\n",
    "filter_parser = PydanticOutputParser(pydantic_object=FilterSpec)\n",
    "\n",
    "def llm_parse_filters_all_fields(\n",
    "    api_key: str,\n",
    "    user_query: str,\n",
    "    model_name: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0\n",
    ") -> FilterSpec:\n",
    "    llm = ChatOpenAI(api_key=api_key, model=model_name, temperature=temperature)\n",
    "    chain = filter_prompt | llm\n",
    "    \n",
    "    # İlk önce raw response alalım\n",
    "    response = chain.invoke({\n",
    "        \"user_query\": user_query,\n",
    "        \"format_instructions\": filter_parser.get_format_instructions()\n",
    "    })\n",
    "    \n",
    "    # Response'u temizleyelim\n",
    "    import json\n",
    "    import re\n",
    "    \n",
    "    content = response.content if hasattr(response, 'content') else str(response)\n",
    "    \n",
    "    # Eğer response bir liste içeriyorsa, son elemanı alalım\n",
    "    try:\n",
    "        parsed = json.loads(content)\n",
    "        if isinstance(parsed, list):\n",
    "            # Liste ise son elemanı (kullanıcının sorgusuna ait olanı) alalım\n",
    "            content = json.dumps(parsed[-1])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # JSON dışındaki metinleri temizle\n",
    "    json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "    if json_match:\n",
    "        content = json_match.group()\n",
    "    \n",
    "    # Şimdi parse edelim\n",
    "    try:\n",
    "        json_obj = json.loads(content)\n",
    "        spec = FilterSpec.model_validate(json_obj)\n",
    "    except Exception as e:\n",
    "        print(f\"Parse hatası: {e}\")\n",
    "        print(f\"Raw content: {content}\")\n",
    "        # Fallback: boş FilterSpec döndür\n",
    "        spec = FilterSpec()\n",
    "\n",
    "    # Anahtar & değer normalizasyonu\n",
    "    def _normalize_dict(d: Dict[str, Union[str, List[str]]]) -> Dict[str, Union[str, List[str]]]:\n",
    "        out: Dict[str, Union[str, List[str]]] = {}\n",
    "        for k, v in d.items():\n",
    "            field = FIELD_MAP.get(k, k)\n",
    "            if isinstance(v, list):\n",
    "                out[field] = [_canon_value(field, x) for x in v]\n",
    "            else:\n",
    "                out[field] = _canon_value(field, v)\n",
    "        return out\n",
    "\n",
    "    def _normalize_ranges(rngs: Dict[str, RangeSpec]) -> Dict[str, RangeSpec]:\n",
    "        out: Dict[str, RangeSpec] = {}\n",
    "        for k, r in rngs.items():\n",
    "            field = FIELD_MAP.get(k, k)\n",
    "            out[field] = r\n",
    "        return out\n",
    "\n",
    "    spec.must = _normalize_dict(spec.must)\n",
    "    spec.must_not = _normalize_dict(spec.must_not)\n",
    "    spec.should = {FIELD_MAP.get(k, k): [_canon_value(FIELD_MAP.get(k, k), x) for x in v]\n",
    "                   for k, v in spec.should.items()}\n",
    "    spec.ranges = _normalize_ranges(spec.ranges)\n",
    "    return spec\n",
    "\n",
    "TEXT_EQ_FIELDS = {\n",
    "    \"marka_key\",\"seri_key\",\"model_key\",\"konum_key\",\"kasa_tipi_key\",\n",
    "    \"cekis_key\",\"renk_key\",\"arac_durumu_key\",\"vites_std\",\"yakit_std\"\n",
    "}\n",
    "NUM_FIELDS = {\"fiyat_num\",\"km_num\",\"yil_num\",\"tramer_num\",\"Araç_Yası\"}\n",
    "\n",
    "def filterspec_to_qdrant(spec: FilterSpec) -> Optional[QFilter]:\n",
    "    must: List[FieldCondition] = []\n",
    "    should: List[FieldCondition] = []\n",
    "    must_not: List[FieldCondition] = []\n",
    "\n",
    "    # MUST koşulları (eşitlik ve range)\n",
    "    for field, val in spec.must.items():\n",
    "        if field in TEXT_EQ_FIELDS:\n",
    "            if isinstance(val, list):\n",
    "                for v in val:\n",
    "                    must.append(FieldCondition(key=field, match=MatchValue(value=str(v))))\n",
    "            else:\n",
    "                must.append(FieldCondition(key=field, match=MatchValue(value=str(val))))\n",
    "        elif field in NUM_FIELDS:\n",
    "            try:\n",
    "                fv = float(val) if not isinstance(val, list) else float(val[0])\n",
    "                must.append(FieldCondition(key=field, range=Range(gte=fv, lte=fv)))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # RANGE koşulları\n",
    "    for field, r in spec.ranges.items():\n",
    "        cond = {}\n",
    "        if r.gte is not None: cond[\"gte\"] = float(r.gte)\n",
    "        if r.lte is not None: cond[\"lte\"] = float(r.lte)\n",
    "        if cond:\n",
    "            must.append(FieldCondition(key=field, range=Range(**cond)))\n",
    "\n",
    "    # MUST NOT koşulları\n",
    "    for field, val in spec.must_not.items():\n",
    "        if field in TEXT_EQ_FIELDS:\n",
    "            if isinstance(val, list):\n",
    "                for v in val:\n",
    "                    must_not.append(FieldCondition(key=field, match=MatchValue(value=str(v))))\n",
    "            else:\n",
    "                must_not.append(FieldCondition(key=field, match=MatchValue(value=str(val))))\n",
    "\n",
    "    # SHOULD koşulları (örnek: birden fazla şehir olabilir)\n",
    "    for field, vals in spec.should.items():\n",
    "        if field in TEXT_EQ_FIELDS:\n",
    "            for v in vals:\n",
    "                should.append(FieldCondition(key=field, match=MatchValue(value=str(v))))\n",
    "\n",
    "    if not (must or should or must_not):\n",
    "        return None\n",
    "    return QFilter(must=must or None, should=should or None, must_not=must_not or None)\n",
    "def make_payload_predicate(spec: FilterSpec):\n",
    "    \"\"\"\n",
    "    Sparse sonuçları yerel olarak süzmek için predicate.\n",
    "    \"\"\"\n",
    "    def ok(pl: Dict[str, Any]) -> bool:\n",
    "        # must (eşitlik)\n",
    "        for field, val in spec.must.items():\n",
    "            pv = str(pl.get(field, \"\")).lower()\n",
    "            if isinstance(val, list):\n",
    "                if pv not in [str(x).lower() for x in val]:\n",
    "                    return False\n",
    "            else:\n",
    "                if pv != str(val).lower():\n",
    "                    return False\n",
    "\n",
    "        # ranges\n",
    "        for field, r in spec.ranges.items():\n",
    "            v = pl.get(field)\n",
    "            if v is None:\n",
    "                return False\n",
    "            try:\n",
    "                vf = float(v)\n",
    "            except:\n",
    "                return False\n",
    "            if r.gte is not None and vf < r.gte: return False\n",
    "            if r.lte is not None and vf > r.lte: return False\n",
    "\n",
    "        # must_not\n",
    "        for field, val in spec.must_not.items():\n",
    "            pv = str(pl.get(field, \"\")).lower()\n",
    "            if isinstance(val, list):\n",
    "                if pv in [str(x).lower() for x in val]:\n",
    "                    return False\n",
    "            else:\n",
    "                if pv == str(val).lower():\n",
    "                    return False\n",
    "\n",
    "        # should: esnek (en az birini sağlasa iyi; sağlamasa da geçir)\n",
    "        # Katılaştırmak istersen, en az bir should alanı sağlanmalı kontrolü ekleyebilirsin.\n",
    "        return True\n",
    "    return ok\n",
    "\n",
    "def search_with_llm_filters(\n",
    "    searcher: 'HybridSearcher',\n",
    "    user_query: str,\n",
    "    api_key: str,\n",
    "    top_k: int = 30,\n",
    "    model_name: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0\n",
    "):\n",
    "    # 1) LLM'den FilterSpec üret\n",
    "    spec = llm_parse_filters_all_fields(\n",
    "        api_key=api_key,\n",
    "        user_query=user_query,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # 2) Qdrant Filter (dense için)\n",
    "    qf = filterspec_to_qdrant(spec)\n",
    "\n",
    "    # 3) Dense: Qdrant + filter; Sparse: yerel predicate\n",
    "    if qf is None:\n",
    "        dense = searcher.dense_search(user_query, f=None, top_k=top_k)\n",
    "    else:\n",
    "        res = searcher.client.search(\n",
    "            collection_name=searcher.collection,\n",
    "            query_vector=searcher.embedder.embed_query(user_query),\n",
    "            query_filter=qf,\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "        )\n",
    "        dense = [(r.id, float(r.score), r.payload or {}) for r in res]\n",
    "\n",
    "    pred = make_payload_predicate(spec)\n",
    "    sparse_all = searcher.sparse_search(user_query, f=None, top_k=top_k * 4)\n",
    "    sparse = [t for t in sparse_all if pred(t[2])]\n",
    "\n",
    "    # 4) RRF birleştir\n",
    "    return searcher.rrf_merge(dense, sparse, top_k=top_k)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 9) (İsteğe bağlı) LLM ile \"madde madde öneri + link\" çıktısı\n",
    "# ==========================================================\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "SELECTION_SYSTEM = \"\"\"Aşağıda bir kullanıcının araç arama isteği ve aday listesi var.\n",
    "Görev: Kullanıcının niyetine EN UYGUN en fazla 5 arabayı seç ve her birini madde işaretleriyle, kısa gerekçelerle ve tıklanabilir bağlantısıyla yaz.\n",
    "Kurallar:\n",
    "- Kullanıcı niyetindeki açık eşiklere (bütçe üst sınırı, yıl alt sınırı, km üst sınırı, tramer) uymayanları ele.\n",
    "- Öncelik: bütçeyi aşmayanlar > yıl/km sınırına uyanlar > vites/yakıt/kasa/şehir uyumu > fiyat/performans.\n",
    "- Format (sadece maddeler):\n",
    "  • {{Baslik/Trim}} | {{Yıl}} | {{Km}} | {{Fiyat}} | {{Yakıt/Vites}} | {{Şehir}} | {{Neden kısa}} | [Link]\n",
    "- Giriş/sonuç paragrafı yazma; sadece maddeler.\n",
    "\"\"\"\n",
    "\n",
    "SELECTION_HUMAN = \"\"\"KULLANICI NİYETİ:\n",
    "{user_query}\n",
    "\n",
    "ADAY LİSTE (<=20) - JSON:\n",
    "{candidates_json}\n",
    "\"\"\"\n",
    "\n",
    "selection_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SELECTION_SYSTEM),\n",
    "    (\"human\", SELECTION_HUMAN),\n",
    "])\n",
    "\n",
    "FIELD_KEEP_FOR_OUTPUT = [\n",
    "    \"id\",\"marka\",\"seri\",\"model\",\"yil_num\",\"km_num\",\"fiyat_num\",\n",
    "    \"vites_tipi\",\"yakit_tipi\",\"kasa_tipi\",\"konum\",\"konum_key\",\"url\",\"baslik\"\n",
    "]\n",
    "\n",
    "def simplify_candidates_for_output(results: List[tuple]) -> List[Dict[str, Any]]:\n",
    "    simplified: List[Dict[str, Any]] = []\n",
    "    for pid, score, pl in results[:20]:\n",
    "        if not isinstance(pl, dict):\n",
    "            continue\n",
    "        row = {\"id\": pid, \"score\": float(score)}\n",
    "        for k in FIELD_KEEP_FOR_OUTPUT:\n",
    "            if k in pl:\n",
    "                row[k] = pl[k]\n",
    "        # eksik sayısalları None bırak (katı eleme yapma)\n",
    "        for f in [\"yil_num\",\"km_num\",\"fiyat_num\"]:\n",
    "            if f not in row or row[f] in (\"\", \"—\"):\n",
    "                row[f] = None\n",
    "        if \"baslik\" in row and isinstance(row[\"baslik\"], str) and len(row[\"baslik\"]) > 120:\n",
    "            row[\"baslik\"] = row[\"baslik\"][:117] + \"...\"\n",
    "        simplified.append(row)\n",
    "    return simplified\n",
    "\n",
    "def llm_selection_text(\n",
    "    api_key: str,\n",
    "    user_query: str,\n",
    "    selection_results: List[tuple],\n",
    "    model_name: str = \"gpt-4o-mini\",\n",
    "    temperature: float = 0.0\n",
    ") -> str:\n",
    "    llm = ChatOpenAI(api_key=api_key, model=model_name, temperature=temperature)\n",
    "    chain = selection_prompt | llm | StrOutputParser()\n",
    "    cands = simplify_candidates_for_output(selection_results)\n",
    "    import json as _json\n",
    "    candidates_json = _json.dumps(cands, ensure_ascii=False)\n",
    "    return chain.invoke({\n",
    "        \"user_query\": user_query,\n",
    "        \"candidates_json\": candidates_json\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbf85de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22380/2225713883.py:426: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  res = self.client.search(\n"
     ]
    }
   ],
   "source": [
    "searcher = HybridSearcher(client, collection, embedder)\n",
    "results = search_with_llm_filters(\n",
    "    searcher,\n",
    "    user_query=\"ASTRA İSTİYORUM HEMEN ÇOK ACİL BANA ASTRA VER YÜZ BİN KM'DEN DÜŞÜK\",\n",
    "    api_key=API_KEY,\n",
    "    top_k=50,\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "# filters = heuristic_parse(user_query)  \n",
    "# results = searcher.search(user_query, None, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61a62c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Union\n",
    "from pydantic import BaseModel, Field, conlist\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf8d5e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Any\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser, StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "# ---------- 1) (Opsiyonel) Yapısal seçim modeli ----------\n",
    "class PickedCar(BaseModel):\n",
    "    id: Union[int, str] = Field(..., description=\"Orijinal point/record id\")\n",
    "    reason: str = Field(..., description=\"Seçme gerekçesi (kısa, somut)\")\n",
    "\n",
    "class Selection(BaseModel):\n",
    "    selected: List[PickedCar] = Field(..., min_length=1, max_length=5)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Selection)\n",
    "\n",
    "# ---------- 2) Sistem ve kullanıcı yönergeleri ----------\n",
    "SYSTEM_STRUCTURED = \"\"\"Aşağıda bir kullanıcının araç arama isteği ve bu isteğe göre bulunan aday arabalar var.\n",
    "Görevin: Kullanıcının niyetine EN UYGUN en fazla 5 arabayı seç ve kısa gerekçeler yaz.\n",
    "Kurallar:\n",
    "- Kullanıcı niyetinden açık eşikler (bütçe üst sınırı, yıl alt sınırı, km üst sınırı) varsa bunları uygula.\n",
    "- Öncelik: bütçeyi aşmayanlar > yıl/ km sınırına uyanlar > vites/yakıt/kasa/şehir uyumu > fiyat/performans.\n",
    "- Uygun aday 5’ten azsa daha az dönebilirsin.\n",
    "- ÇIKTIYI sadece {format_name} şemasına UYUMLU JSON olarak ver. Serbest metin yazma.\n",
    "\"\"\"\n",
    "\n",
    "HUMAN_STRUCTURED = \"\"\"KULLANICI NİYETİ:\n",
    "{user_query}\n",
    "\n",
    "ADAY LİSTE (<=20):\n",
    "{candidates_json}\n",
    "\n",
    "ÇIKTI ŞEMASI:\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt_structured = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_STRUCTURED),\n",
    "    (\"human\", HUMAN_STRUCTURED),\n",
    "])\n",
    "\n",
    "# ---------- 3) Serbest metin (kullanıcıya gösterilecek) ----------\n",
    "SYSTEM_TEXT = \"\"\"Aşağıda bir kullanıcının araç arama isteği ve aday listesi var.\n",
    "Görev: Kullanıcının niyetine EN UYGUN en fazla 5 arabayı seç ve bunları liste gibi değil, doğal akışta bir satış asistanı gibi anlat.\n",
    "Kurallar:\n",
    "- Kullanıcı niyetindeki açık eşiklere (bütçe üst sınırı, yıl alt sınırı, km üst sınırı) uymayanları ele.\n",
    "- Öncelik: bütçeyi aşmayanlar > yıl/km sınırına uyanlar > vites/yakıt/kasa/şehir uyumu > fiyat/performans.\n",
    "- Eğer uygun aday 5’ten azsa daha az dönebilirsin.\n",
    "- Üslup: Samimi, ikna edici ve yardımcı. Açıklamalar doğal dilde olacak, araç özelliklerini cümle içinde bütünleştir.\n",
    "- Format:\n",
    "  - Önce kısa bir giriş paragrafı (“Sizin için birkaç seçenek buldum” gibi).\n",
    "  - Her aracı ayrı bir paragrafta tanıt. Örn: “2018 model Opel Astra 92 bin kilometrede, otomatik ve benzinli olmasıyla öne çıkıyor. Fiyatı 1 milyon 245 bin TL ve İstanbul’da. Bu aracın en büyük avantajı…” \n",
    "  - Aracın sonunda linki doğal cümlenin içine göm (“Detaylara buradan ulaşabilirsiniz: [Link]”).\n",
    "  - Son olarak küçük bir kapanış paragrafı ekle (“İsterseniz bu seçeneklerden başlayabiliriz” gibi).\n",
    "\"\"\"\n",
    "\n",
    "HUMAN_TEXT = \"\"\"KULLANICI NİYETİ:\n",
    "{user_query}\n",
    "\n",
    "ADAY LİSTE (<=20) - JSON:\n",
    "{candidates_json}\n",
    "\"\"\"\n",
    "\n",
    "prompt_text = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_TEXT),\n",
    "    (\"human\", HUMAN_TEXT),\n",
    "])\n",
    "\n",
    "# ---------- 4) LLM kurucu yardımcılar ----------\n",
    "def build_llm(api_key: str, model_name: str = \"gpt-4o-mini\", temperature: float = 0.0) -> ChatOpenAI:\n",
    "    return ChatOpenAI(api_key=api_key, model=model_name, temperature=temperature)\n",
    "\n",
    "def build_selector_llm_structured(api_key: str, model_name: str = \"gpt-4o-mini\", temperature: float = 0.0):\n",
    "    llm = build_llm(api_key, model_name, temperature)\n",
    "    # Yapısal seçim (Pydantic) için zincir\n",
    "    chain = prompt_structured | llm | parser\n",
    "    return chain\n",
    "\n",
    "def build_selector_llm_text(api_key: str, model_name: str = \"gpt-4o-mini\", temperature: float = 0.0):\n",
    "    llm = build_llm(api_key, model_name, temperature)\n",
    "    # Serbest metin (tam LLM cevabı) için zincir\n",
    "    chain = prompt_text | llm | StrOutputParser()\n",
    "    return chain\n",
    "\n",
    "# ---------- 5) Aday sadeleştirme + filtreler ----------\n",
    "FIELD_KEEP = [\n",
    "    \"id\",\"marka\",\"seri\",\"model\",\"yil_num\",\"km_num\",\"fiyat_num\",\n",
    "    \"vites_tipi\",\"yakit_tipi\",\"kasa_tipi\",\"sehir_key\",\"konum\",\"url\",\"baslik\"\n",
    "]\n",
    "\n",
    "REQUIRED_NUMERIC = [\"yil_num\", \"km_num\", \"fiyat_num\"]  # bu alanlar dolu/parse edilebilir olmalı\n",
    "\n",
    "def is_valid_candidate(pl: Dict[str, Any]) -> bool:\n",
    "    for f in REQUIRED_NUMERIC:\n",
    "        if f not in pl or pl[f] in (None, \"\", \"—\"):\n",
    "            return False\n",
    "        # sayısal parse edilemiyorsa ele\n",
    "        try:\n",
    "            float(pl[f])\n",
    "        except Exception:\n",
    "            return False\n",
    "    # URL/başlık da kontrol edilebilir\n",
    "    if not pl.get(\"url\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def simplify_candidates(results: list) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    results: HybridSearcher.search(...) çıktısı gibi (pid, score, payload) tuple listesi bekler.\n",
    "    İlk 20 kaydı alır, zorunlu alanları kontrol eder, gösterim için kısaltır.\n",
    "    \"\"\"\n",
    "    simplified: List[Dict[str, Any]] = []\n",
    "    for pid, score, pl in results[:20]:\n",
    "        if not isinstance(pl, dict):\n",
    "            continue\n",
    "        if not is_valid_candidate(pl):\n",
    "            continue\n",
    "        row = {\"id\": pid, \"score\": float(score)}\n",
    "        for k in FIELD_KEEP:\n",
    "            if k in pl:\n",
    "                row[k] = pl[k]\n",
    "        # başlık çok uzunsa kısalt\n",
    "        if \"baslik\" in row and isinstance(row[\"baslik\"], str) and len(row[\"baslik\"]) > 120:\n",
    "            row[\"baslik\"] = row[\"baslik\"][:117] + \"...\"\n",
    "        simplified.append(row)\n",
    "    return simplified\n",
    "\n",
    "# ---------- 6) Dış API çağrıları ----------\n",
    "\n",
    "# Not: Burada API_KEY'in global'de tanımlı olduğu varsayılmıştır.\n",
    "# API_KEY = \"...\"  # kendi anahtarınızı sağlayın\n",
    "\n",
    "def llm_select_top5_structured(user_query: str, results: list, api_key: str, model_name: str = \"gpt-4o-mini\") -> Selection:\n",
    "    \"\"\"(Opsiyonel) JSON-uyumlu yapısal seçim döner.\"\"\"\n",
    "    chain = build_selector_llm_structured(api_key=api_key, model_name=model_name)\n",
    "    cands = simplify_candidates(results)\n",
    "    candidates_json = json.dumps(cands, ensure_ascii=False)\n",
    "    out: Selection = chain.invoke({\n",
    "        \"user_query\": user_query,\n",
    "        \"candidates_json\": candidates_json,\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "        \"format_name\": \"Selection\"\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def llm_select_top5_text(user_query: str, results: list, api_key: str, model_name: str = \"gpt-4o-mini\") -> str:\n",
    "    \"\"\"\n",
    "    Kullanıcıya gösterilecek TAM METİN (madde madde, neden + link) döner.\n",
    "    Bu zincir, Pydantic parse ETMEZ; LLM'nin ürettiği tüm metin aynen gelir.\n",
    "    \"\"\"\n",
    "    chain = build_selector_llm_text(api_key=api_key, model_name=model_name)\n",
    "    cands = simplify_candidates(results)\n",
    "    candidates_json = json.dumps(cands, ensure_ascii=False)\n",
    "    text: str = chain.invoke({\n",
    "        \"user_query\": user_query,\n",
    "        \"candidates_json\": candidates_json\n",
    "    })\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "947880dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizin için birkaç seçenek buldum. Hemen Astra arayışınıza uygun olanları inceleyelim.\n",
      "\n",
      "İlk olarak, 2009 model Opel Astra 1.3 CDTI Essentia dikkat çekiyor. Bu araç, 345 bin kilometrede ve otomatik vites seçeneği ile geliyor. Fiyatı 399 bin TL olan bu Astra, hem konforlu bir sürüş sunuyor hem de dizel motoruyla yakıt tasarrufu sağlıyor. Diyarbakır'da bulunan bu aracı daha detaylı incelemek isterseniz, buradan ulaşabilirsiniz: [Detaylar](https://www.arabam.com/ilan/galeriden-satilik-opel-astra-1-3-cdti-essentia/hp-motor-s-dan-2009-astra-otomatik-bayan-araci/31915490).\n",
      "\n",
      "Bir diğer seçenek ise 2013 model Opel Astra 1.3 CDTI Edition. Bu araç 313 bin kilometrede ve düz vites seçeneği ile geliyor. Fiyatı 590 bin TL olan bu Astra, hem şık tasarımı hem de performansıyla öne çıkıyor. Konya'da bulunan bu araca göz atmak isterseniz, detaylara buradan ulaşabilirsiniz: [Detaylar](https://www.arabam.com/ilan/galeriden-satilik-opel-astra-1-3-cdti-edition/hatasiz-boyasiz-opel-astra/32862417).\n",
      "\n",
      "Maalesef, bütçeniz ve kilometre sınırınıza uyan başka bir Astra bulamadım. Ancak bu iki seçenek, ihtiyaçlarınıza uygun ve hemen değerlendirebileceğiniz araçlar. İsterseniz bu seçeneklerden başlayabiliriz.\n"
     ]
    }
   ],
   "source": [
    "full_text = llm_select_top5_text(\n",
    "    user_query=\"ASTRA İSTİYORUM HEMEN ÇOK ACİL BANA ASTRA VER YÜZ BİN KM'DEN DÜŞÜK\",\n",
    "    results=results,\n",
    "    api_key=API_KEY,\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "print(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d29e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d10b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
