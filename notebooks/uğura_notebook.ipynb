{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8ffdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/train/.local/lib/python3.12/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unidecode in /home/train/.local/lib/python3.12/site-packages (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /home/train/.local/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /home/train/.local/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/train/.local/lib/python3.12/site-packages (from sentence-transformers) (4.12.2)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/train/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/train/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/train/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/train/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/train/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/train/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/train/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/train/.local/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
      "Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Using cached huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "Using cached hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:13:09\u001b[0mm0:00:01\u001b[0m00:15\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:09:43\u001b[0mm0:00:01\u001b[0m00:12\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m  \u001b[33m0:00:58\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:08:41\u001b[0mm0:00:01\u001b[0m00:12\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:02:25\u001b[0mm0:00:01\u001b[0m00:04\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:52\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m852.6 kB/s\u001b[0m  \u001b[33m0:03:35\u001b[0m0:00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m796.1 kB/s\u001b[0m  \u001b[33m0:05:30\u001b[0m0:00:01\u001b[0m0:07\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:05:08\u001b[0mm0:00:01\u001b[0m00:07\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m870.2 kB/s\u001b[0m  \u001b[33m0:07:08\u001b[0m0:00:01\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m514.6 kB/s\u001b[0m  \u001b[33m0:01:07\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:02:32\u001b[0mm0:00:01\u001b[0m00:04\u001b[0mm\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m987.2 kB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m817.6 kB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, threadpoolctl, sympy, setuptools, scipy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, joblib, hf-xet, fsspec, filelock, triton, scikit-learn, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/35\u001b[0m [sentence-transformers]ence-transformers]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.19.1 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.1 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 regex-2025.9.18 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence-transformers-5.1.1 setuptools-80.9.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.8.0 transformers-4.56.2 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/train/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 0) Qdrant bağlantısı\\nclient = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\\n\\n# 1) DataFrame yükle ve normalize et\\ndf = pd.read_parquet(\"ilanlar.parquet\")   # veya csv\\ndf = normalize_df(df)\\n\\n# 2) Embedder (CPU)\\nembedder = ST_Embedder(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", force_device=\"cpu\")\\n\\n# 3) Upsert\\ncollection = \"car_listings_st\"   # 384-dim modeller için ayrı koleksiyon önerilir\\ndf_to_points(df, embedder, collection, client, batch_size=256)\\n\\n# 4) Arama\\nsearcher = HybridSearcher(client, collection, embedder)\\nuser_query = \"İstanbul’da 1.3 milyon TL’ye kadar, 2018 üzeri otomatik benzinli Astra\"\\nfilters = heuristic_parse(user_query)  # veya LLM tabanlı parser\\nresults = searcher.search(user_query, filters, top_k=20)\\n\\n# results: [(id, score, payload), ...]\\n# payload[\\'marka\\'], payload[\\'model\\'], payload[\\'yil_num\\'], payload[\\'fiyat_num\\'], payload[\\'url\\'] ...\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# car_vector_search.py\n",
    "# Gereksinimler:\n",
    "# pip install pandas qdrant-client sentence-transformers scikit-learn unidecode pydantic tqdm\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%pip install tqdm\n",
    "from tqdm import tqdm\n",
    "%pip install unidecode\n",
    "from unidecode import unidecode\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance,\n",
    "    VectorParams,\n",
    "    PointStruct,\n",
    "    Filter,\n",
    "    FieldCondition,\n",
    "    Range,\n",
    "    MatchValue,\n",
    ")\n",
    "\n",
    "%pip install sentence-transformers\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Embedder (Sentence-Transformers - CPU varsayılan)\n",
    "# -----------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class ST_Embedder:\n",
    "    \"\"\"\n",
    "    Sentence-Transformers tabanlı embedder.\n",
    "    - Varsayılan cihaz: CPU\n",
    "    - normalize_embeddings=True (cosine için iyi pratik)\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "                 force_device: str = \"cpu\"):\n",
    "        self.device = force_device\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        try:\n",
    "            return self.model.encode(\n",
    "                texts,\n",
    "                convert_to_numpy=True,\n",
    "                batch_size=64,\n",
    "                normalize_embeddings=True,\n",
    "                show_progress_bar=False,\n",
    "            ).tolist()\n",
    "        except Exception:\n",
    "            # GPU'da sorun olursa CPU'ya düş\n",
    "            if self.device != \"cpu\":\n",
    "                self.device = \"cpu\"\n",
    "                self.model = SentenceTransformer(self.model_name, device=\"cpu\")\n",
    "                return self.model.encode(\n",
    "                    texts,\n",
    "                    convert_to_numpy=True,\n",
    "                    batch_size=64,\n",
    "                    normalize_embeddings=True,\n",
    "                    show_progress_bar=False,\n",
    "                ).tolist()\n",
    "            raise\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "    def dimension(self) -> int:\n",
    "        if hasattr(self.model, \"get_sentence_embedding_dimension\"):\n",
    "            return self.model.get_sentence_embedding_dimension()\n",
    "        # çok nadir durumda:\n",
    "        return len(self.embed_query(\"test\"))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Yardımcılar / Normalizasyon\n",
    "# -----------------------------\n",
    "TURKISH_MAP = {\n",
    "    \"otomatik\": [\"otomatik\", \"auto\", \"dct\", \"edc\", \"e-cvt\", \"cvt\", \"tiptronic\", \"multitronic\", \"dsg\"],\n",
    "    \"manuel\": [\"manuel\", \"manual\"],\n",
    "    \"benzin\": [\"benzin\", \"gasoline\", \"benzinli\"],\n",
    "    \"dizel\": [\"dizel\", \"diesel\"],\n",
    "    \"lpg\": [\"lpg\", \"autogas\"],\n",
    "    \"hybrid\": [\"hibrid\", \"hybrid\"],\n",
    "    \"elektrik\": [\"elektrik\", \"electric\", \"bev\", \"ev\"],\n",
    "}\n",
    "\n",
    "def ascii_lower(s: Any) -> str:\n",
    "    return unidecode(str(s or \"\")).strip().lower()\n",
    "\n",
    "def to_num(text: Any) -> Optional[float]:\n",
    "    if text is None:\n",
    "        return None\n",
    "    s = str(text).strip().lower()\n",
    "    if s in (\"nan\", \"\", \"none\", \"yok\", \"—\", \"-\"):\n",
    "        return None\n",
    "    s = s.replace(\"tl\", \"\").replace(\"₺\", \"\").replace(\"km\", \"\")\n",
    "    s = s.replace(\"milyon\", \"000000\").replace(\"mn\", \"000000\").replace(\"m\", \"000000\")\n",
    "    s = re.sub(\n",
    "        r\"(\\d+)[\\.,]?(\\d*)\\s*bin\",\n",
    "        lambda m: str(float(m.group(1) + \".\" + (m.group(2) or \"0\")) * 1000),\n",
    "        s,\n",
    "    )\n",
    "    s = s.replace(\".\", \"\").replace(\" \", \"\")\n",
    "    s = s.replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def year4(x: Any) -> Optional[int]:\n",
    "    m = re.search(r\"\\b(19|20)\\d{2}\\b\", str(x or \"\"))\n",
    "    return int(m.group(0)) if m else None\n",
    "\n",
    "def none_if_nan(x: Any):\n",
    "    try:\n",
    "        return None if (x is None or (isinstance(x, float) and math.isnan(x))) else x\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "def make_point_id(raw: Any):\n",
    "    \"\"\"\n",
    "    Qdrant ID: unsigned int veya UUID string olmalı.\n",
    "    \"\"\"\n",
    "    if raw is None:\n",
    "        return str(uuid.uuid4())\n",
    "    # int?\n",
    "    try:\n",
    "        if isinstance(raw, float):\n",
    "            if math.isnan(raw):\n",
    "                return str(uuid.uuid4())\n",
    "            if float(raw).is_integer() and int(raw) >= 0:\n",
    "                return int(raw)\n",
    "            return str(uuid.uuid4())\n",
    "        iv = int(raw)\n",
    "        if iv >= 0:\n",
    "            return iv\n",
    "    except Exception:\n",
    "        pass\n",
    "    # UUID?\n",
    "    try:\n",
    "        return str(uuid.UUID(str(raw)))\n",
    "    except Exception:\n",
    "        return str(uuid.uuid4())\n",
    "\n",
    "def match_from_map(value: str, mapping: Dict[str, List[str]]) -> str:\n",
    "    v = ascii_lower(value)\n",
    "    for canon, variants in mapping.items():\n",
    "        for t in variants:\n",
    "            if t in v:\n",
    "                return canon\n",
    "    return value\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) DataFrame Normalize\n",
    "# -----------------------------\n",
    "def normalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    expected = [\n",
    "        \"id\",\"baslik\",\"konum\",\"fiyat\",\"aciklama\",\"marka\",\"seri\",\"model\",\"yil\",\"kilometre\",\n",
    "        \"yakit_tipi\",\"vites_tipi\",\"renk\",\"arac_durumu\",\"kasa_tipi\",\"cekis\",\"motor_hacmi\",\n",
    "        \"motor_gucu\",\"tramer\",\"url\"\n",
    "    ]\n",
    "    missing = [c for c in expected if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Eksik kolon(lar): {missing}\")\n",
    "\n",
    "    # metin\n",
    "    for col in [\n",
    "        \"baslik\",\"konum\",\"aciklama\",\"marka\",\"seri\",\"model\",\"yakit_tipi\",\"vites_tipi\",\n",
    "        \"renk\",\"arac_durumu\",\"kasa_tipi\",\"cekis\",\"url\"\n",
    "    ]:\n",
    "        df[col] = df[col].map(lambda x: re.sub(r\"\\s+\", \" \", str(x or \"\").strip()))\n",
    "\n",
    "    # sayısal\n",
    "    df[\"fiyat_num\"] = df[\"fiyat\"].map(to_num)\n",
    "    df[\"km_num\"] = df[\"kilometre\"].map(to_num)\n",
    "    df[\"tramer_num\"] = df[\"tramer\"].map(to_num)\n",
    "    df[\"yil_num\"] = df[\"yil\"].map(year4)\n",
    "\n",
    "    # vites / yakıt std\n",
    "    df[\"vites_std\"] = df[\"vites_tipi\"].apply(\n",
    "        lambda v: match_from_map(v, {\"otomatik\": TURKISH_MAP[\"otomatik\"], \"manuel\": TURKISH_MAP[\"manuel\"]})\n",
    "    )\n",
    "    def yakit_std(v):\n",
    "        if match_from_map(v, {\"benzin\": TURKISH_MAP[\"benzin\"]}) == \"benzin\": return \"benzin\"\n",
    "        if match_from_map(v, {\"dizel\": TURKISH_MAP[\"dizel\"]}) == \"dizel\": return \"dizel\"\n",
    "        if match_from_map(v, {\"lpg\": TURKISH_MAP[\"lpg\"]}) == \"lpg\": return \"lpg\"\n",
    "        if match_from_map(v, {\"hybrid\": TURKISH_MAP[\"hybrid\"]}) == \"hybrid\": return \"hybrid\"\n",
    "        if match_from_map(v, {\"elektrik\": TURKISH_MAP[\"elektrik\"]}) == \"elektrik\": return \"elektrik\"\n",
    "        return ascii_lower(v)\n",
    "    df[\"yakit_std\"] = df[\"yakit_tipi\"].map(yakit_std)\n",
    "\n",
    "    # arama anahtarları\n",
    "    for col in [\"marka\",\"seri\",\"model\",\"konum\",\"kasa_tipi\",\"cekis\",\"renk\",\"arac_durumu\"]:\n",
    "        df[col + \"_key\"] = df[col].map(ascii_lower)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Doc metni + payload\n",
    "# -----------------------------\n",
    "def build_doc_text(row: Dict[str, Any]) -> str:\n",
    "    marka = str(row.get(\"marka\", \"\")).strip()\n",
    "    seri = str(row.get(\"seri\", \"\")).strip()\n",
    "    model = str(row.get(\"model\", \"\")).strip()\n",
    "    yil = row.get(\"yil_num\") or row.get(\"yil\") or \"\"\n",
    "    vites = row.get(\"vites_tipi\", \"\")\n",
    "    yakit = row.get(\"yakit_tipi\", \"\")\n",
    "    km = row.get(\"kilometre\", \"\")\n",
    "    konum = row.get(\"konum\", \"\")\n",
    "    kasa = row.get(\"kasa_tipi\", \"\")\n",
    "    fiyat = row.get(\"fiyat\", \"\")\n",
    "    aciklama = (row.get(\"aciklama\") or \"\").strip()\n",
    "\n",
    "    title = f\"{marka} {seri} {model} {yil}\".strip()\n",
    "    bullet = f\"{yakit}, {vites}, {km} km, {kasa}, {konum}\".replace(\"  \",\" \").strip(\" ,\")\n",
    "    text = f\"{title} – {bullet}. Fiyat: {fiyat}. {aciklama}\"\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def build_payload(r: Dict[str, Any], text: str) -> Dict[str, Any]:\n",
    "    return {\n",
    "        # ham alanlar\n",
    "        \"id\": r.get(\"id\"),\n",
    "        \"baslik\": r.get(\"baslik\"),\n",
    "        \"konum\": r.get(\"konum\"),\n",
    "        \"fiyat\": r.get(\"fiyat\"),\n",
    "        \"marka\": r.get(\"marka\"),\n",
    "        \"seri\": r.get(\"seri\"),\n",
    "        \"model\": r.get(\"model\"),\n",
    "        \"yil\": r.get(\"yil_num\") or r.get(\"yil\"),\n",
    "        \"kilometre\": r.get(\"km_num\") or r.get(\"kilometre\"),\n",
    "        \"yakit_tipi\": r.get(\"yakit_tipi\"),\n",
    "        \"vites_tipi\": r.get(\"vites_tipi\"),\n",
    "        \"renk\": r.get(\"renk\"),\n",
    "        \"arac_durumu\": r.get(\"arac_durumu\"),\n",
    "        \"kasa_tipi\": r.get(\"kasa_tipi\"),\n",
    "        \"cekis\": r.get(\"cekis\"),\n",
    "        \"motor_hacmi\": r.get(\"motor_hacmi\"),\n",
    "        \"motor_gucu\": r.get(\"motor_gucu\"),\n",
    "        \"tramer\": r.get(\"tramer\"),\n",
    "        \"url\": r.get(\"url\"),\n",
    "\n",
    "        # sayısal & key alanlar\n",
    "        \"fiyat_num\": none_if_nan(r.get(\"fiyat_num\")),\n",
    "        \"km_num\": none_if_nan(r.get(\"km_num\")),\n",
    "        \"yil_num\": none_if_nan(r.get(\"yil_num\")),\n",
    "        \"marka_key\": ascii_lower(r.get(\"marka\")),\n",
    "        \"seri_key\": ascii_lower(r.get(\"seri\")),\n",
    "        \"model_key\": ascii_lower(r.get(\"model\")),\n",
    "        \"konum_key\": ascii_lower(r.get(\"konum\")),\n",
    "\n",
    "        # arama metni\n",
    "        \"text\": text,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Qdrant collection garanti\n",
    "# -----------------------------\n",
    "def ensure_collection(client: QdrantClient, collection: str, dim: int, distance: Distance = Distance.COSINE):\n",
    "    existing = [c.name for c in client.get_collections().collections]\n",
    "    if collection in existing:\n",
    "        # Boyut uyuşmasını kontrol et (Qdrant sürümüne göre alanlar değişebilir)\n",
    "        info = client.get_collection(collection)\n",
    "        # Bazı sürümlerde: info.config.params.vectors.size\n",
    "        current_dim = None\n",
    "        try:\n",
    "            current_dim = info.config.params.vectors.size  # type: ignore\n",
    "        except Exception:\n",
    "            # Yedek yol: vektör sayısı boyut değil, o yüzden kullanma\n",
    "            pass\n",
    "        if current_dim is not None and current_dim != dim:\n",
    "            raise ValueError(f\"Koleksiyon '{collection}' farklı boyutta: {current_dim} ≠ {dim}\")\n",
    "        return\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection,\n",
    "        vectors_config=VectorParams(size=dim, distance=distance),\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) DF → Qdrant Upsert\n",
    "# -----------------------------\n",
    "def df_to_points(df: pd.DataFrame, embedder: ST_Embedder, collection: str,\n",
    "                 client: QdrantClient, batch_size: int = 256):\n",
    "    if \"fiyat_num\" not in df.columns:\n",
    "        df[\"fiyat_num\"] = df[\"fiyat\"].map(to_num)\n",
    "    if \"km_num\" not in df.columns:\n",
    "        df[\"km_num\"] = df[\"kilometre\"].map(to_num)\n",
    "    if \"yil_num\" not in df.columns:\n",
    "        df[\"yil_num\"] = df[\"yil\"].map(year4)\n",
    "\n",
    "    dim = embedder.dimension()\n",
    "    ensure_collection(client, collection, dim)\n",
    "\n",
    "    rows = df.to_dict(orient=\"records\")\n",
    "    for i in tqdm(range(0, len(rows), batch_size), desc=\"upserting\"):\n",
    "        chunk = rows[i:i + batch_size]\n",
    "        texts = [build_doc_text(r) for r in chunk]\n",
    "        vecs = embedder.embed_documents(texts)\n",
    "\n",
    "        points = []\n",
    "        for r, v, t in zip(chunk, vecs, texts):\n",
    "            pid = make_point_id(r.get(\"id\"))\n",
    "            payload = build_payload(r, t)\n",
    "            points.append(PointStruct(id=pid, vector=v, payload=payload))\n",
    "\n",
    "        client.upsert(collection_name=collection, points=points)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Filtre modeli (sorgu → payload filter)\n",
    "# -----------------------------\n",
    "class QueryFilters(BaseModel):\n",
    "    marka: Optional[str] = None\n",
    "    seri: Optional[str] = None\n",
    "    model: Optional[str] = None\n",
    "    konum: Optional[str] = None\n",
    "    fiyat_min: Optional[float] = None\n",
    "    fiyat_max: Optional[float] = None\n",
    "    yil_min: Optional[int] = None\n",
    "    yil_max: Optional[int] = None\n",
    "\n",
    "def build_qdrant_filter(f: QueryFilters) -> Optional[Filter]:\n",
    "    must: List[FieldCondition] = []\n",
    "\n",
    "    def eq(field: str, val: Optional[str]):\n",
    "        val = (val or \"\").strip().lower()\n",
    "        if val:\n",
    "            must.append(FieldCondition(key=field, match=MatchValue(value=val)))\n",
    "\n",
    "    def rng(field: str, gte=None, lte=None):\n",
    "        cond = {}\n",
    "        if gte is not None: cond[\"gte\"] = float(gte)\n",
    "        if lte is not None: cond[\"lte\"] = float(lte)\n",
    "        if cond:\n",
    "            must.append(FieldCondition(key=field, range=Range(**cond)))\n",
    "\n",
    "    eq(\"marka_key\", f.marka)\n",
    "    eq(\"seri_key\",  f.seri)\n",
    "    eq(\"model_key\", f.model)\n",
    "    eq(\"konum_key\", f.konum)\n",
    "    rng(\"fiyat_num\", gte=f.fiyat_min, lte=f.fiyat_max)\n",
    "    rng(\"yil_num\",   gte=f.yil_min,   lte=f.yil_max)\n",
    "\n",
    "    return Filter(must=must) if must else None\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) HybridSearcher (Dense + Sparse TF-IDF + RRF)\n",
    "# -----------------------------\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def _is_valid_text(s: str, min_len: int = 3) -> bool:\n",
    "    return isinstance(s, str) and len(s.strip()) >= min_len\n",
    "\n",
    "class HybridSearcher:\n",
    "    def __init__(self, client: QdrantClient, collection: str, embedder: ST_Embedder):\n",
    "        self.client = client\n",
    "        self.collection = collection\n",
    "        self.embedder = embedder\n",
    "        self._tfidf: Optional[TfidfVectorizer] = None\n",
    "        self._sparse = None\n",
    "        self._ids: Optional[np.ndarray] = None   # dtype=object\n",
    "        self._texts: Optional[List[str]] = None\n",
    "        self._payloads: Dict[Any, Dict] = {}     # id -> payload (cache)\n",
    "\n",
    "    def _ensure_sparse_index(self, max_points: int = 20000):\n",
    "        if self._tfidf is not None:\n",
    "            return\n",
    "\n",
    "        texts: List[str] = []\n",
    "        ids:   List[Any] = []\n",
    "        seen = set()\n",
    "        next_offset = None\n",
    "\n",
    "        while True:\n",
    "            recs, next_offset = self.client.scroll(\n",
    "                collection_name=self.collection,\n",
    "                with_payload=True,\n",
    "                limit=1024,\n",
    "                offset=next_offset,\n",
    "            )\n",
    "            if not recs:\n",
    "                break\n",
    "\n",
    "            for p in recs:\n",
    "                pid = p.id  # tipini KORU (int ya da str-UUID)\n",
    "                if pid in seen:\n",
    "                    continue\n",
    "                pl = p.payload or {}\n",
    "                t = pl.get(\"text\", \"\")\n",
    "                if _is_valid_text(t):\n",
    "                    texts.append(t)\n",
    "                    ids.append(pid)\n",
    "                    self._payloads[pid] = pl\n",
    "                    seen.add(pid)\n",
    "\n",
    "            if next_offset is None or len(texts) >= max_points:\n",
    "                break\n",
    "\n",
    "        if not texts:\n",
    "            texts = [\"placeholder\"]\n",
    "            ids = [\"__placeholder__\"]\n",
    "            self._payloads[\"__placeholder__\"] = {\"text\": \"placeholder\"}\n",
    "\n",
    "        self._tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1, 2))\n",
    "        self._sparse = self._tfidf.fit_transform(texts)\n",
    "        self._ids = np.array(ids, dtype=object)\n",
    "        self._texts = texts\n",
    "\n",
    "    def dense_search(self, query: str, f: Optional[QueryFilters], top_k: int = 50):\n",
    "        qv = self.embedder.embed_query(query)\n",
    "        qf = build_qdrant_filter(f) if f else None\n",
    "        res = self.client.search(\n",
    "            collection_name=self.collection,\n",
    "            query_vector=qv,\n",
    "            query_filter=qf,\n",
    "            limit=top_k,\n",
    "            with_payload=True,\n",
    "            with_vectors=False,\n",
    "        )\n",
    "        return [(r.id, float(r.score), r.payload or {}) for r in res]\n",
    "\n",
    "    def sparse_search(self, query: str, f: Optional[QueryFilters], top_k: int = 200):\n",
    "        self._ensure_sparse_index()\n",
    "        q = self._tfidf.transform([query])\n",
    "        sim = (q @ self._sparse.T).toarray().ravel()\n",
    "\n",
    "        k = min(top_k, sim.size)\n",
    "        if k == 0:\n",
    "            return []\n",
    "        idx = np.argpartition(-sim, k - 1)[:k]\n",
    "        order = idx[np.argsort(-sim[idx])]\n",
    "\n",
    "        results = []\n",
    "        for i in order:\n",
    "            pid = self._ids[i]\n",
    "            sc = float(sim[i])\n",
    "            pl = self._payloads.get(pid, {})\n",
    "\n",
    "            # Payload filtreleri\n",
    "            if f:\n",
    "                if f.marka and (pl.get(\"marka_key\") or \"\") != (f.marka or \"\").strip().lower():  continue\n",
    "                if f.seri  and (pl.get(\"seri_key\")  or \"\") != (f.seri  or \"\").strip().lower():  continue\n",
    "                if f.model and (pl.get(\"model_key\") or \"\") != (f.model or \"\").strip().lower():  continue\n",
    "                if f.konum and (pl.get(\"konum_key\") or \"\") != (f.konum or \"\").strip().lower():  continue\n",
    "                if f.fiyat_min is not None and (pl.get(\"fiyat_num\") is None or pl[\"fiyat_num\"] < f.fiyat_min): continue\n",
    "                if f.fiyat_max is not None and (pl.get(\"fiyat_num\") is None or pl[\"fiyat_num\"] > f.fiyat_max): continue\n",
    "                if f.yil_min  is not None and (pl.get(\"yil_num\")   is None or pl[\"yil_num\"]   < f.yil_min):  continue\n",
    "                if f.yil_max  is not None and (pl.get(\"yil_num\")   is None or pl[\"yil_num\"]   > f.yil_max):  continue\n",
    "\n",
    "            results.append((pid, sc, pl))\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def rrf_merge(dense: List[Tuple[Any, float, Dict]], sparse: List[Tuple[Any, float, Dict]],\n",
    "                  k: float = 60.0, top_k: int = 50):\n",
    "        def ranks(lst):\n",
    "            return {pid: rank for rank, (pid, _, _) in enumerate(sorted(lst, key=lambda x: -x[1]), start=1)}\n",
    "        rd = ranks(dense)\n",
    "        rs = ranks(sparse)\n",
    "        ids = set([pid for pid, _, _ in dense] + [pid for pid, _, _ in sparse])\n",
    "        merged = []\n",
    "        for pid in ids:\n",
    "            r1 = rd.get(pid, 10**6)\n",
    "            r2 = rs.get(pid, 10**6)\n",
    "            rrf = 1.0 / (k + r1) + 1.0 / (k + r2)\n",
    "            payload = None\n",
    "            if pid in rd:\n",
    "                payload = [pl for (p, _, pl) in dense if p == pid][0]\n",
    "            elif pid in rs:\n",
    "                payload = [pl for (p, _, pl) in sparse if p == pid][0]\n",
    "            merged.append((pid, rrf, payload or {}))\n",
    "        merged.sort(key=lambda x: -x[1])\n",
    "        return merged[:top_k]\n",
    "\n",
    "    def search(self, query_text: str, f: Optional[QueryFilters] = None, top_k: int = 30):\n",
    "        dense = self.dense_search(query_text, f, top_k=top_k)\n",
    "        sparse = self.sparse_search(query_text, f, top_k=top_k * 4)\n",
    "        return self.rrf_merge(dense, sparse, top_k=top_k)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 8) (Opsiyonel) Heuristik parser örneği\n",
    "# -----------------------------\n",
    "def heuristic_parse(user_text: str) -> QueryFilters:\n",
    "    s = ascii_lower(user_text)\n",
    "    # fiyat max\n",
    "    fiyat_max = None\n",
    "    m = re.search(r\"(\\d[\\d\\.\\, ]+)\\s*(tl|₺|try|lira)?\\s*(?:max|üst|tavan|kadar|altında|aşmadan)\", s)\n",
    "    if not m:\n",
    "        m = re.search(r\"maks(?:imum)?\\s*(\\d[\\d\\.\\, ]+)\", s)\n",
    "    if m: fiyat_max = to_num(m.group(1))\n",
    "\n",
    "    # yıl min\n",
    "    yil_min = None\n",
    "    mm = re.findall(r\"\\b(19|20)\\d{2}\\b\", s)\n",
    "    if mm:\n",
    "        try:\n",
    "            ys = re.findall(r\"\\b(19|20)\\d{2}\\b\", s)\n",
    "            yil_min = int(re.search(r\"\\b(19|20)\\d{2}\\b\", s).group(0))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # konum basit\n",
    "    konum = None\n",
    "    m = re.search(r\"(istanbul|ankara|izmir|bursa|antalya|adana|konya|kayseri|kocaeli|mersin|gaziantep)\", s)\n",
    "    if m: konum = m.group(1)\n",
    "\n",
    "    # marka/seri/model çok basit ipuçları (ör: Astra)\n",
    "    marka = None\n",
    "    seri = None\n",
    "    if \"astra\" in s:\n",
    "        marka = \"opel\"\n",
    "        seri = \"astra\"\n",
    "\n",
    "    return QueryFilters(marka=marka, seri=seri, konum=konum, fiyat_max=fiyat_max, yil_min=yil_min)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Örnek kullanım (çalıştırmayın)\n",
    "# -----------------------------\n",
    "\"\"\"\n",
    "# 0) Qdrant bağlantısı\n",
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)\n",
    "\n",
    "# 1) DataFrame yükle ve normalize et\n",
    "df = pd.read_parquet(\"ilanlar.parquet\")   # veya csv\n",
    "df = normalize_df(df)\n",
    "\n",
    "# 2) Embedder (CPU)\n",
    "embedder = ST_Embedder(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\", force_device=\"cpu\")\n",
    "\n",
    "# 3) Upsert\n",
    "collection = \"car_listings_st\"   # 384-dim modeller için ayrı koleksiyon önerilir\n",
    "df_to_points(df, embedder, collection, client, batch_size=256)\n",
    "\n",
    "# 4) Arama\n",
    "searcher = HybridSearcher(client, collection, embedder)\n",
    "user_query = \"İstanbul’da 1.3 milyon TL’ye kadar, 2018 üzeri otomatik benzinli Astra\"\n",
    "filters = heuristic_parse(user_query)  # veya LLM tabanlı parser\n",
    "results = searcher.search(user_query, filters, top_k=20)\n",
    "\n",
    "# results: [(id, score, payload), ...]\n",
    "# payload['marka'], payload['model'], payload['yil_num'], payload['fiyat_num'], payload['url'] ...\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc210b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(url=\"http://localhost:6333\", prefer_grpc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "868ab421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Veri boyutu: (65839, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>baslik</th>\n",
       "      <th>konum</th>\n",
       "      <th>fiyat</th>\n",
       "      <th>aciklama</th>\n",
       "      <th>marka</th>\n",
       "      <th>seri</th>\n",
       "      <th>model</th>\n",
       "      <th>yil</th>\n",
       "      <th>kilometre</th>\n",
       "      <th>yakit_tipi</th>\n",
       "      <th>vites_tipi</th>\n",
       "      <th>renk</th>\n",
       "      <th>arac_durumu</th>\n",
       "      <th>kasa_tipi</th>\n",
       "      <th>cekis</th>\n",
       "      <th>motor_hacmi</th>\n",
       "      <th>motor_gucu</th>\n",
       "      <th>url</th>\n",
       "      <th>Araç_Yası</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>YAVUZLAR'DAN 2014 ALFA ROMEO GİULİETTA 1.6 JTD...</td>\n",
       "      <td>Karşıyaka Mh. Kepez, Antalya</td>\n",
       "      <td>655000</td>\n",
       "      <td>\\n</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Giulietta</td>\n",
       "      <td>1.6 JTD Distinctive</td>\n",
       "      <td>2014</td>\n",
       "      <td>209000</td>\n",
       "      <td>Dizel</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Beyaz</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Hatchback/5</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1598</td>\n",
       "      <td>105</td>\n",
       "      <td>https://www.arabam.com/ilan/galeriden-satilik-...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2006 ALFA ROMEO 156 TS</td>\n",
       "      <td>Soğanlı Mh. Osmangazi, Bursa</td>\n",
       "      <td>414500</td>\n",
       "      <td>ES ES OTOMOTİV DEN \\n\\n\\n \\n\\n\\nSATILIK \\n\\n\\n...</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>156</td>\n",
       "      <td>1.6 TS Distinctive</td>\n",
       "      <td>2006</td>\n",
       "      <td>171000</td>\n",
       "      <td>Benzin</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Şampanya</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1600</td>\n",
       "      <td>125</td>\n",
       "      <td>https://www.arabam.com/ilan/galeriden-satilik-...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Sahibinden Alfa Romeo Giulietta 1.4 TB MultiAi...</td>\n",
       "      <td>Karşıyaka Mh. Karataş, Adana</td>\n",
       "      <td>735000</td>\n",
       "      <td>Ev almayı düşündüğüm için aracımı satışa çıkar...</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Giulietta</td>\n",
       "      <td>1.4 TB MultiAir Distinctive</td>\n",
       "      <td>2011</td>\n",
       "      <td>157100</td>\n",
       "      <td>Benzin</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Siyah</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Hatchback/5</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1368</td>\n",
       "      <td>170</td>\n",
       "      <td>https://www.arabam.com/ilan/sahibinden-satilik...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Sahibinden Alfa Romeo Giulietta 1.4 TB Progres...</td>\n",
       "      <td>Esenkent Mh. Esenyurt, İstanbul</td>\n",
       "      <td>900000</td>\n",
       "      <td>-2016 NİSAN ÇIKIŞLIDIR.-ORJİNAL 90 BİN KM, SIF...</td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Giulietta</td>\n",
       "      <td>1.4 TB Progression Plus</td>\n",
       "      <td>2015</td>\n",
       "      <td>90800</td>\n",
       "      <td>Benzin</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Kırmızı</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Hatchback/5</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1368</td>\n",
       "      <td>120</td>\n",
       "      <td>https://www.arabam.com/ilan/sahibinden-satilik...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A L F İ S T</td>\n",
       "      <td>Deliktaş Mh. Pamukkale, Denizli</td>\n",
       "      <td>800000</td>\n",
       "      <td></td>\n",
       "      <td>Alfa Romeo</td>\n",
       "      <td>Giulietta</td>\n",
       "      <td>1.6 JTD Distinctive</td>\n",
       "      <td>2014</td>\n",
       "      <td>200000</td>\n",
       "      <td>Dizel</td>\n",
       "      <td>Düz</td>\n",
       "      <td>Beyaz</td>\n",
       "      <td>İkinci El</td>\n",
       "      <td>Hatchback/5</td>\n",
       "      <td>Önden Çekiş</td>\n",
       "      <td>1598</td>\n",
       "      <td>105</td>\n",
       "      <td>https://www.arabam.com/ilan/sahibinden-satilik...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             baslik  \\\n",
       "0   1  YAVUZLAR'DAN 2014 ALFA ROMEO GİULİETTA 1.6 JTD...   \n",
       "1   2                             2006 ALFA ROMEO 156 TS   \n",
       "2   3  Sahibinden Alfa Romeo Giulietta 1.4 TB MultiAi...   \n",
       "3   4  Sahibinden Alfa Romeo Giulietta 1.4 TB Progres...   \n",
       "4   5                                        A L F İ S T   \n",
       "\n",
       "                             konum   fiyat  \\\n",
       "0     Karşıyaka Mh. Kepez, Antalya  655000   \n",
       "1     Soğanlı Mh. Osmangazi, Bursa  414500   \n",
       "2     Karşıyaka Mh. Karataş, Adana  735000   \n",
       "3  Esenkent Mh. Esenyurt, İstanbul  900000   \n",
       "4  Deliktaş Mh. Pamukkale, Denizli  800000   \n",
       "\n",
       "                                            aciklama       marka       seri  \\\n",
       "0                                                 \\n  Alfa Romeo  Giulietta   \n",
       "1  ES ES OTOMOTİV DEN \\n\\n\\n \\n\\n\\nSATILIK \\n\\n\\n...  Alfa Romeo        156   \n",
       "2  Ev almayı düşündüğüm için aracımı satışa çıkar...  Alfa Romeo  Giulietta   \n",
       "3  -2016 NİSAN ÇIKIŞLIDIR.-ORJİNAL 90 BİN KM, SIF...  Alfa Romeo  Giulietta   \n",
       "4                                                     Alfa Romeo  Giulietta   \n",
       "\n",
       "                         model   yil  kilometre yakit_tipi vites_tipi  \\\n",
       "0          1.6 JTD Distinctive  2014     209000      Dizel        Düz   \n",
       "1           1.6 TS Distinctive  2006     171000     Benzin        Düz   \n",
       "2  1.4 TB MultiAir Distinctive  2011     157100     Benzin        Düz   \n",
       "3      1.4 TB Progression Plus  2015      90800     Benzin        Düz   \n",
       "4          1.6 JTD Distinctive  2014     200000      Dizel        Düz   \n",
       "\n",
       "       renk arac_durumu    kasa_tipi        cekis  motor_hacmi  motor_gucu  \\\n",
       "0     Beyaz   İkinci El  Hatchback/5  Önden Çekiş         1598         105   \n",
       "1  Şampanya   İkinci El        Sedan  Önden Çekiş         1600         125   \n",
       "2     Siyah   İkinci El  Hatchback/5  Önden Çekiş         1368         170   \n",
       "3   Kırmızı   İkinci El  Hatchback/5  Önden Çekiş         1368         120   \n",
       "4     Beyaz   İkinci El  Hatchback/5  Önden Çekiş         1598         105   \n",
       "\n",
       "                                                 url  Araç_Yası  \n",
       "0  https://www.arabam.com/ilan/galeriden-satilik-...         11  \n",
       "1  https://www.arabam.com/ilan/galeriden-satilik-...         19  \n",
       "2  https://www.arabam.com/ilan/sahibinden-satilik...         14  \n",
       "3  https://www.arabam.com/ilan/sahibinden-satilik...         10  \n",
       "4  https://www.arabam.com/ilan/sahibinden-satilik...         11  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uğur burda df'i okut\n",
    "df = pd.read_parquet(\"../data/arabam_ilanlar.parquet\")\n",
    "\n",
    "print(\"✅ Veri boyutu:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c43132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3259cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ST_Embedder(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"car_listings_st\"   # 384-dim modeller için ayrı koleksiyon önerilir\n",
    "df_to_points(df, embedder, collection, client, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afccb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = HybridSearcher(client, collection, embedder)\n",
    "user_query = \"İstanbul’da 1.3 milyon TL’ye kadar, 2018 üzeri otomatik benzinli Astra\"\n",
    "filters = heuristic_parse(user_query)  \n",
    "results = searcher.search(user_query, filters, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a5ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
